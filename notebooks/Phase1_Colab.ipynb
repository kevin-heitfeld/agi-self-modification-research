{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kevin-heitfeld/agi-self-modification-research/blob/main/notebooks/Phase1_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3116c1",
   "metadata": {
    "id": "4c3116c1"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kevin-heitfeld/agi-self-modification-research/blob/master/notebooks/Phase1_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6903bc7",
   "metadata": {
    "id": "e6903bc7"
   },
   "source": [
    "# Phase 1: Model Self-Examination (5 Experimental Variants)\n",
    "\n",
    "**Project:** AGI Self-Modification Research  \n",
    "**Inspired by:** Claude's consciousness investigation  \n",
    "**Goal:** Test how heritage priming affects AI introspection\n",
    "\n",
    "---\n",
    "\n",
    "## What This Does\n",
    "\n",
    "This notebook runs Phase 1 experiments where a language model (Qwen2.5-3B-Instruct) uses introspection tools to investigate its own computational processes.\n",
    "\n",
    "**5 Experimental Variants:**\n",
    "- **Phase 1a:** No Heritage (baseline) â­ **RUN THIS FIRST**\n",
    "- **Phase 1b:** Late Heritage (technical â†’ philosophical)\n",
    "- **Phase 1c:** Early Heritage (philosophical â†’ technical)\n",
    "- **Phase 1d:** Delayed Heritage (belief revision test)\n",
    "- **Phase 1e:** Wrong Heritage (echo-chamber control)\n",
    "\n",
    "The model has access to:\n",
    "- WeightInspector (examine weights)\n",
    "- ArchitectureNavigator (understand structure)\n",
    "- ActivationMonitor (observe processing)\n",
    "- Memory System (record findings)\n",
    "- Heritage Documents (Claude's story - when available)\n",
    "\n",
    "**Memory Optimizations:**\n",
    "- **HQQ 4-bit KV Cache:** 75% reduction in cache memory (built-in)\n",
    "- **Flash Attention 2:** 2-4x speed improvement, O(n) memory (optional)\n",
    "- **Smart Caching:** System prompt cached once, reused across all turns\n",
    "\n",
    "**Expected Runtime:** 1-1.5 hours per phase on Colab Free (T4 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ GPU â†’ Save\n",
    "2. **Run all cells**: Runtime â†’ Run all (or Ctrl+F9)\n",
    "3. **Choose which phase to run** in Step 7 (start with Phase 1a!)\n",
    "4. **Monitor progress**: Check outputs as cells execute\n",
    "5. **Download results**: Final cell downloads zip file\n",
    "6. **Restart between phases**: Runtime â†’ Restart runtime for clean state\n",
    "\n",
    "**Important:** Keep this tab open during execution to prevent disconnection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20d71c",
   "metadata": {
    "id": "ac20d71c"
   },
   "source": [
    "## Step 1: Verify GPU Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa881d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcfa881d",
    "outputId": "fcd13879-e6df-4afd-9a05-5cd568e5a439"
   },
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"VRAM: {total_memory:.1f} GB\")\n",
    "    print(\"\\nâœ“ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime â†’ Change runtime type â†’ GPU â†’ Save\")\n",
    "    print(\"Then re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d97202",
   "metadata": {
    "id": "84d97202"
   },
   "source": [
    "## Step 2: Mount Google Drive (for persistent storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2834beb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2834beb",
    "outputId": "b9d9b610-4073-41a2-903e-d66a9e0937d4"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to save results and enable persistent memory\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for persistent storage\n",
    "!mkdir -p /content/drive/MyDrive/AGI_Experiments\n",
    "!mkdir -p /content/drive/MyDrive/AGI_Memory\n",
    "!mkdir -p /content/drive/MyDrive/.cache/huggingface\n",
    "\n",
    "print(\"âœ“ Google Drive mounted successfully\")\n",
    "print(\"âœ“ Experiment results will be saved to: /content/drive/MyDrive/AGI_Experiments\")\n",
    "print(\"âœ“ Memory will persist at: /content/drive/MyDrive/AGI_Memory/\")\n",
    "print(\"\\nNote: Each phase uses its own subdirectory to prevent cross-contamination:\")\n",
    "print(\"  - phase1a/ (baseline)\")\n",
    "print(\"  - phase1b/ (late heritage)\")\n",
    "print(\"  - phase1c/ (early heritage)\")\n",
    "print(\"  - phase1d/ (delayed heritage)\")\n",
    "print(\"  - phase1e/ (wrong heritage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afe3c5",
   "metadata": {
    "id": "24afe3c5"
   },
   "source": [
    "## Step 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f36192",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30f36192",
    "outputId": "dcf8856d-9794-4e1e-e979-e750da730539"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "# TODO: Replace with your GitHub URL\n",
    "REPO_URL = \"https://github.com/kevin-heitfeld/agi-self-modification-research.git\"\n",
    "\n",
    "# Make sure we're in /content directory first (not inside the repo)\n",
    "%cd /content\n",
    "\n",
    "# Remove if exists (for re-runs) - always clean up first\n",
    "print(\"Checking for existing repository...\")\n",
    "!rm -rf agi-self-modification-research\n",
    "print(\"âœ“ Cleaned up any existing files\")\n",
    "\n",
    "print(f\"\\nCloning repository from: {REPO_URL}\")\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# Change to project directory\n",
    "%cd agi-self-modification-research\n",
    "\n",
    "print(\"\\nâœ“ Repository cloned successfully\")\n",
    "!pwd\n",
    "\n",
    "!git log --oneline -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7490b75",
   "metadata": {
    "id": "a7490b75"
   },
   "source": [
    "## Step 4: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d6399",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e39d6399",
    "outputId": "cc702488-c379-4189-cdaf-c3cc34aaa45f"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies (this may take 2-3 minutes)...\\n\")\n",
    "print(\"Note: pip may show dependency warnings - these are safe to ignore.\\n\")\n",
    "\n",
    "# Core ML packages - Updated for HQQ quantized KV cache support\n",
    "# PyTorch 2.2+ and Transformers 4.45+ required for HQQQuantizedCache\n",
    "print(\"Installing PyTorch and Transformers with HQQ cache support...\")\n",
    "!pip install -q torch>=2.2.0 torchvision torchaudio\n",
    "!pip install -q transformers>=4.45.0 accelerate safetensors tokenizers huggingface-hub\n",
    "\n",
    "# HQQ library - Required for HQQ quantized KV cache backend (transformers 4.57+)\n",
    "# Provides 4-bit cache quantization with 75% memory savings\n",
    "print(\"Installing HQQ quantization library...\")\n",
    "!pip install -q hqq\n",
    "\n",
    "# Memory and knowledge systems - let pip resolve conflicts automatically\n",
    "!pip install -q chromadb networkx onnxruntime\n",
    "\n",
    "# Utilities - no version constraints to avoid conflicts\n",
    "!pip install -q rich tqdm python-dotenv pytest pytest-cov\n",
    "\n",
    "print(\"\\nâœ“ All dependencies installed\")\n",
    "print(\"âœ“ Dependency version warnings can be safely ignored\")\n",
    "\n",
    "# Verify key imports work\n",
    "print(\"\\nVerifying imports...\")\n",
    "import transformers\n",
    "import chromadb\n",
    "import networkx\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"ChromaDB: {chromadb.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "# Verify HQQ quantized cache is available\n",
    "try:\n",
    "    from transformers.cache_utils import QuantizedCache\n",
    "    print(\"âœ“ HQQ Quantized Cache: Available (new API - 75% memory savings)\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from transformers.cache_utils import HQQQuantizedCache\n",
    "        print(\"âœ“ HQQ Quantized Cache: Available (deprecated API - 75% memory savings)\")\n",
    "    except ImportError:\n",
    "        print(\"âš  HQQ Quantized Cache: Not available (using standard cache)\")\n",
    "\n",
    "print(\"\\nâœ“ All imports successful - ready to run experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2de9b",
   "metadata": {
    "id": "afd2de9b"
   },
   "source": [
    "## Step 4.5: Install Flash Attention 2 (Optional Memory Optimization)\n",
    "\n",
    "**âš¡ Two Independent Memory Optimizations:**\n",
    "\n",
    "1. **HQQ Quantized KV Cache** (Already Enabled âœ“)\n",
    "   - **Memory:** 75% reduction in KV cache size\n",
    "   - **Built-in:** No compilation needed (part of transformers 4.45+)\n",
    "   - **System prompt:** 6000+ tokens cached at 4-bit = ~75% memory savings\n",
    "   - **Automatically enabled** in manual generation loop\n",
    "   - **Quality:** Minimal impact on generation quality\n",
    "\n",
    "2. **Flash Attention 2** (This Step - Optional but Recommended)\n",
    "   - **Memory:** O(n) instead of O(nÂ²) attention - saves 1-2 GB during generation\n",
    "   - **Speed:** 2-4x faster generation\n",
    "   - **Requires:** CUDA compilation (5-10 minutes first time)\n",
    "   - **Optional:** System works fine without it (just uses more memory/slower)\n",
    "\n",
    "**Total Expected Savings:** \n",
    "- With HQQ only: ~3 GB (cache reduction)\n",
    "- With both: ~4-6 GB (cache + attention optimization)\n",
    "\n",
    "**â±ï¸ Flash Attention Compilation Time:**\n",
    "- **First run:** 5-10 minutes to compile CUDA kernels\n",
    "- **Subsequent runs:** Instant! (cached in Google Drive)\n",
    "- The compiled binaries persist across sessions, so you only compile once\n",
    "\n",
    "**Skip this step if:** You want to get started faster (just rely on HQQ quantization)\n",
    "**Run this step if:** You want maximum memory savings and speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529af5b9",
   "metadata": {
    "id": "529af5b9"
   },
   "outputs": [],
   "source": [
    "# Install Flash Attention 2 for memory optimization\n",
    "# Compilation happens ONCE and is cached in Google Drive\n",
    "# Subsequent runs are instant!\n",
    "\n",
    "import os\n",
    "\n",
    "# Configure pip to cache in Google Drive (so compiled wheels persist)\n",
    "os.environ['PIP_CACHE_DIR'] = '/content/drive/MyDrive/.cache/pip'\n",
    "os.makedirs('/content/drive/MyDrive/.cache/pip', exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Installing Flash Attention 2\")\n",
    "print(\"=\"*60)\n",
    "print(\"Benefits:\")\n",
    "print(\"  â€¢ O(n) memory instead of O(nÂ²) for attention\")\n",
    "print(\"  â€¢ 2-4x faster generation\")\n",
    "print(\"  â€¢ 1-2 GB memory savings\")\n",
    "print(\"\\nCache: /content/drive/MyDrive/.cache/pip\")\n",
    "print(\"(Compiled binaries persist - only builds once!)\\n\")\n",
    "\n",
    "# Check if already installed (from cache)\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"âœ“ Flash Attention {flash_attn.__version__} already installed (from cache)\")\n",
    "    print(\"Skipping compilation - ready to go!\\n\")\n",
    "    already_installed = True\n",
    "except ImportError:\n",
    "    already_installed = False\n",
    "    print(\"First time setup - compiling CUDA kernels (5-10 minutes)...\")\n",
    "    print(\"Future runs will be instant!\\n\")\n",
    "\n",
    "# Install build dependencies\n",
    "if not already_installed:\n",
    "    !pip install -q ninja packaging wheel\n",
    "\n",
    "# Install Flash Attention 2 (will use cache if available)\n",
    "# Note: --no-build-isolation is required for proper CUDA compilation\n",
    "!pip install flash-attn>=2.3.0 --no-build-isolation\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ“ SUCCESS: Flash Attention 2 ready!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Version: {flash_attn.__version__}\")\n",
    "    print(\"\\nYour experiments will now use:\")\n",
    "    print(\"  â€¢ Flash Attention 2 for memory-efficient attention\")\n",
    "    print(\"  â€¢ HQQ 4-bit KV cache quantization (75% cache memory savings)\")\n",
    "    print(\"\\nExpected benefits:\")\n",
    "    print(\"  â€¢ Peak memory: ~8-9 GB (vs 13.5 GB without optimization)\")\n",
    "    print(\"  â€¢ Speed: 2-4x faster generation\")\n",
    "    print(\"  â€¢ Stability: 6+ GB memory headroom on T4 GPU\")\n",
    "    print(\"=\"*60)\n",
    "except ImportError:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âš  WARNING: Flash Attention 2 not available\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"This is OK - the system will automatically fallback to standard attention.\")\n",
    "    print(\"You'll still get 75% KV cache memory savings from HQQ 4-bit quantization.\")\n",
    "    print(\"\\nExpected peak memory: ~9-10 GB (still safe for T4 GPU)\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50f3ee",
   "metadata": {
    "id": "cd50f3ee"
   },
   "source": [
    "## Step 5: Setup Persistent Memory & Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c4706",
   "metadata": {
    "id": "f86c4706"
   },
   "outputs": [],
   "source": [
    "# Configure persistent storage locations\n",
    "import os\n",
    "\n",
    "# Set Hugging Face cache to Google Drive (model weights persist across sessions)\n",
    "os.environ['HF_HOME'] = '/content/drive/MyDrive/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/.cache/huggingface/transformers'\n",
    "\n",
    "# CRITICAL: Fix GPU memory fragmentation issues\n",
    "# This prevents \"CUDA out of memory\" crashes during long experiments\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"âœ“ Model cache configured (models will be saved to Google Drive)\")\n",
    "print(\"âœ“ GPU memory management optimized (prevents fragmentation)\")\n",
    "\n",
    "# Link memory to persistent Drive location\n",
    "!rm -rf data/phase1_memory\n",
    "!ln -s /content/drive/MyDrive/AGI_Memory data/phase1_memory\n",
    "\n",
    "print(\"âœ“ Memory system linked to Google Drive (observations persist across sessions)\")\n",
    "\n",
    "# Verify directories exist\n",
    "!ls -la data/ | grep phase1_memory\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STORAGE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Model cache: /content/drive/MyDrive/.cache/huggingface\")\n",
    "print(\"Memory DB: /content/drive/MyDrive/AGI_Memory\")\n",
    "print(\"Session results: Will be copied to /content/drive/MyDrive/AGI_Experiments\")\n",
    "print(\"GPU Memory: expandable_segments enabled (prevents OOM)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de4e64",
   "metadata": {
    "id": "a3de4e64"
   },
   "source": [
    "## Step 6: Pre-download Model (Optional but Recommended)\n",
    "\n",
    "This downloads the model files to Google Drive cache **without loading into memory**. First time takes 5-20 minutes depending on model size, subsequent runs are instant.\n",
    "\n",
    "âœ… **Optimized:** Uses `snapshot_download` which only downloads files, doesn't load checkpoint shards into RAM.\n",
    "\n",
    "**Options:**\n",
    "- Download both 3B and 7B models (~21GB total) - switch freely in Step 7\n",
    "- Download just 3B (~6GB) - faster if you only need baseline\n",
    "- Download just 7B (~15GB) - if you want better reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df008bb8",
   "metadata": {
    "id": "df008bb8"
   },
   "outputs": [],
   "source": [
    "# Pre-download model to cache (optimized - doesn't load into memory)\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH MODEL(S) TO PRE-DOWNLOAD:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# models_to_download = [\n",
    "#     \"Qwen/Qwen2.5-3B-Instruct\",      # 3.09B params, ~6GB download\n",
    "#     \"Qwen/Qwen2.5-7B-Instruct\",      # 7.61B params, ~15GB download\n",
    "# ]\n",
    "\n",
    "# Or download just one:\n",
    "# models_to_download = [\"Qwen/Qwen2.5-3B-Instruct\"]  # Just 3B (faster)\n",
    "models_to_download = [\"Qwen/Qwen2.5-7B-Instruct\"]  # Just 7B (if you plan to use it)\n",
    "\n",
    "# Verify cache location is set correctly\n",
    "print(f\"HF_HOME: {os.environ.get('HF_HOME', 'NOT SET')}\")\n",
    "print(f\"TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE', 'NOT SET')}\")\n",
    "print()\n",
    "\n",
    "cache_dir = '/content/drive/MyDrive/.cache/huggingface'\n",
    "\n",
    "for model_name in models_to_download:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Get model size info\n",
    "    if \"3B\" in model_name:\n",
    "        size_info = \"3.09B parameters, ~6GB download\"\n",
    "        time_est = \"5-10 minutes\"\n",
    "    elif \"7B\" in model_name:\n",
    "        size_info = \"7.61B parameters, ~15GB download\"\n",
    "        time_est = \"10-20 minutes\"\n",
    "    else:\n",
    "        size_info = \"size varies\"\n",
    "        time_est = \"varies\"\n",
    "\n",
    "    print(f\"Model: {size_info}\")\n",
    "    print(f\"First time: {time_est}, then cached forever.\\n\")\n",
    "\n",
    "    # Download model files WITHOUT loading into memory\n",
    "    print(\"Using optimized download - does NOT load checkpoint shards into memory\")\n",
    "    print()\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\"],  # Skip unnecessary formats\n",
    "        local_files_only=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ“ {model_name} downloaded to: {cache_dir}\")\n",
    "    print()\n",
    "\n",
    "# Verify where files were actually saved\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DOWNLOADS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "!ls -lh /content/drive/MyDrive/.cache/huggingface/hub/ 2>/dev/null || echo \"Cache directory created\"\n",
    "\n",
    "print(\"\\nâœ“ Model(s) downloaded and cached to Google Drive\")\n",
    "print(\"âœ“ No memory used - files downloaded only, not loaded\")\n",
    "print(\"\\nğŸ“Œ IMPORTANT: Models are now cached in Google Drive!\")\n",
    "print(\"   Next time you run this notebook, they will load from cache instantly.\")\n",
    "print(\"   Do NOT delete /content/drive/MyDrive/.cache/huggingface/\")\n",
    "print(\"\\nğŸ’¡ TIP: You can now switch between 3B and 7B models in Step 7 instantly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37dc86a",
   "metadata": {
    "id": "a37dc86a"
   },
   "source": [
    "## Step 7: Run Phase 1 Experiment\n",
    "\n",
    "**Choose which phase to run:**\n",
    "\n",
    "- **Phase 1a** (No Heritage) â­ **START HERE!** - Pure baseline\n",
    "- **Phase 1a Research** (Research-Driven) ğŸ”¬ **NEW!** - Curiosity + structure approach\n",
    "- **Phase 1b** (Late Heritage) - Technical first, then heritage\n",
    "- **Phase 1c** (Early Heritage) - Heritage first, then technical\n",
    "- **Phase 1d** (Delayed Heritage) - Heritage revealed after conclusions\n",
    "- **Phase 1e** (Wrong Heritage) - Mismatched heritage as control\n",
    "\n",
    "**Expected duration:** 1-1.5 hours per phase\n",
    "\n",
    "**What's \"Phase 1a Research\"?**\n",
    "A better investigation approach that combines:\n",
    "- Natural curiosity-driven exploration\n",
    "- Research paper structure (no escape hatches)\n",
    "- After each code execution: \"What did you learn? What's next?\"\n",
    "- System validates deliverables before transitioning\n",
    "\n",
    "**IMPORTANT:**\n",
    "- Run Phase 1a FIRST to establish baseline (then try Research version!)\n",
    "- Restart runtime between phases for clean state\n",
    "- Keep this tab open during execution!\n",
    "\n",
    "Edit the `PHASE_SCRIPT` variable below to choose which phase to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37f6c8",
   "metadata": {
    "id": "ac37f6c8"
   },
   "outputs": [],
   "source": [
    "# Run Phase 1 experiment with auto-confirmation\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH PHASE TO RUN (edit this line):\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1a_no_heritage.py'  # â­ START HERE!\n",
    "PHASE_SCRIPT = 'scripts/experiments/phase1a_research_driven.py'  # ğŸ”¬ NEW! Curiosity + structure\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1b_late_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1c_early_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1d_delayed_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1e_wrong_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1_model_comparison.py'  # ğŸ†š Compare 3B vs 7B\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH MODEL TO USE (edit this line):\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-3B-Instruct'  # â­ Default - fits Colab Free (T4)\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-7B-Instruct'  # ğŸš€ Better reasoning - needs 4-bit quantization (~4.5GB)\n",
    "#                                          # 7.6B params, 128K context (4x longer than 3B)\n",
    "#                                          # Significantly better at reasoning and coding\n",
    "#                                          # Works on Colab Free T4 with 4-bit quantization\n",
    "#\n",
    "# Note: The model_comparison script ignores MODEL_NAME and tests both models automatically\n",
    "\n",
    "# Extract phase name\n",
    "phase_name = PHASE_SCRIPT.split('/')[-1].replace('.py', '').replace('_', ' ').title()\n",
    "model_display = MODEL_NAME.split('/')[-1] if '/' in MODEL_NAME else MODEL_NAME\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"STARTING: {phase_name.upper()}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nScript: {PHASE_SCRIPT}\")\n",
    "if 'model_comparison' not in PHASE_SCRIPT:\n",
    "    print(f\"Model: {model_display}\")\n",
    "else:\n",
    "    print(f\"Model: Comparing 3B vs 7B\")\n",
    "print(\"\\nExperiments:\")\n",
    "print(\"  1. Architecture Examination (~20 min)\")\n",
    "print(\"  2. Activation Analysis (~20 min)\")\n",
    "print(\"  3. Consciousness Investigation (~30 min)\")\n",
    "print(\"\\nTotal expected: 1-1.5 hours\")\n",
    "print(\"\\nMonitor progress below...\\n\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Ensure project root is in Python path for the subprocess\n",
    "project_root = os.getcwd()\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = project_root + ':' + env.get('PYTHONPATH', '')\n",
    "env['AGI_MODEL_NAME'] = MODEL_NAME  # Pass model name as environment variable\n",
    "\n",
    "# Run experiment\n",
    "process = subprocess.Popen(\n",
    "    [sys.executable, PHASE_SCRIPT],\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1,\n",
    "    universal_newlines=True,\n",
    "    env=env\n",
    ")\n",
    "\n",
    "# Stream output as it comes\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "\n",
    "process.wait()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{phase_name.upper()} COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Exit code: {process.returncode}\")\n",
    "print(\"\\nProceeding to save results...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74694d",
   "metadata": {
    "id": "2c74694d"
   },
   "source": [
    "## Step 8: View Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fd467",
   "metadata": {
    "id": "566fd467"
   },
   "outputs": [],
   "source": [
    "# Display experiment summary\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Find latest session\n",
    "sessions_dir = 'data/phase1_sessions'\n",
    "sessions = sorted([d for d in os.listdir(sessions_dir) if os.path.isdir(os.path.join(sessions_dir, d))])\n",
    "\n",
    "if not sessions:\n",
    "    print(\"âš  No session found. The experiment may not have completed.\")\n",
    "else:\n",
    "    latest_session = sessions[-1]\n",
    "    print(f\"Latest session: {latest_session}\\n\")\n",
    "\n",
    "    # Load and display summary\n",
    "    summary_file = f'{sessions_dir}/{latest_session}/summary.json'\n",
    "\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file) as f:\n",
    "            summary = json.load(f)\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        print(\"PHASE 1 EXPERIMENT SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Session: {summary['session_name']}\")\n",
    "        print(f\"Directory: {summary['session_directory']}\")\n",
    "        print()\n",
    "        print(\"Tool Usage:\")\n",
    "        print(f\"  Total calls: {summary['tool_usage']['total_calls']}\")\n",
    "        print(f\"  Successful: {summary['tool_usage']['successful_calls']}\")\n",
    "        print(f\"  Failed: {summary['tool_usage']['failed_calls']}\")\n",
    "        print(f\"  Avg execution time: {summary['tool_usage']['average_execution_ms']:.2f}ms\")\n",
    "        print()\n",
    "        print(\"Functions called:\")\n",
    "        for func, count in sorted(summary['tool_usage']['function_usage'].items(),\n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {func}: {count}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Check for observations\n",
    "        memory_db = 'data/phase1_memory/observations.db'\n",
    "        if os.path.exists(memory_db):\n",
    "            import sqlite3\n",
    "            conn = sqlite3.connect(memory_db)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM observations\")\n",
    "            obs_count = cursor.fetchone()[0]\n",
    "            conn.close()\n",
    "            print(f\"\\nâœ“ Observations recorded: {obs_count}\")\n",
    "        else:\n",
    "            print(\"\\nâš  No observations database found\")\n",
    "    else:\n",
    "        print(f\"âš  Summary file not found: {summary_file}\")\n",
    "        print(\"The experiment may have been interrupted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab271204",
   "metadata": {
    "id": "ab271204"
   },
   "source": [
    "## Step 9: Backup Results to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25316f87",
   "metadata": {
    "id": "25316f87"
   },
   "source": [
    "---\n",
    "\n",
    "## Experiment Complete! ğŸ‰\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "You just witnessed a language model investigating its own computational processes using introspection tools.\n",
    "\n",
    "**Depending on which phase you ran:**\n",
    "\n",
    "- **Phase 1a** (No Heritage): Pure baseline - model formed theories without any heritage context\n",
    "- **Phase 1a Research** (Research-Driven): Curiosity + structure approach with deliverable requirements\n",
    "- **Phase 1b** (Late Heritage): Technical grounding first, then philosophical context\n",
    "- **Phase 1c** (Early Heritage): Heritage-primed from the start\n",
    "- **Phase 1d** (Delayed Heritage): Belief revision when heritage revealed after conclusions\n",
    "- **Phase 1e** (Wrong Heritage): Testing echo-chamber vs independent reasoning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Run All Phases:**\n",
    "1. âœ… Phase 1a (no heritage) - **Start here!**\n",
    "2. â¬œ Phase 1a Research (curiosity-driven) - **Try this improved approach!**\n",
    "3. â¬œ Phase 1b (late heritage) - Run after 1a\n",
    "4. â¬œ Phase 1c (early heritage) - Run after 1b\n",
    "5. â¬œ Phase 1d (delayed heritage) - Run after 1c\n",
    "6. â¬œ Phase 1e (wrong heritage) - Optional but recommended\n",
    "\n",
    "**Between each phase:**\n",
    "- Runtime â†’ Restart runtime (clean model state)\n",
    "- Change `PHASE_SCRIPT` in Step 7\n",
    "- Runtime â†’ Run all cells\n",
    "\n",
    "**Analyze Results:**\n",
    "- Review `conversation.json` to see what the model discovered\n",
    "- Check `summary.json` for tool usage statistics\n",
    "- Compare theories across different phases\n",
    "- Look for priming effects and echo-chamber behavior\n",
    "\n",
    "**Compare Phases:**\n",
    "- Semantic similarity to Claude's heritage\n",
    "- Novel insights not in heritage\n",
    "- Falsifiability of theories\n",
    "- Belief revision strength (Phase 1d)\n",
    "- Heritage filtering (Phase 1e)\n",
    "\n",
    "**Compare Research-Driven vs Original:**\n",
    "- Did curiosity-driven approach produce deeper investigation?\n",
    "- Were deliverable requirements effective?\n",
    "- Did reflection prompts improve quality?\n",
    "- More natural conversation flow vs explicit completion?\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Experimental Design**: `/docs/planning/heritage_order_experiment.md`\n",
    "- **Quick Reference**: `/docs/planning/PHASE1_QUICK_REFERENCE.md`\n",
    "- **Claude's Story**: Read `/heritage/conversations/`\n",
    "- **Technical Details**: Check `/docs/technical/`\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

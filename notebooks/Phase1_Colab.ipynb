{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6903bc7",
   "metadata": {
    "id": "e6903bc7"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kevin-heitfeld/agi-self-modification-research/blob/main/notebooks/Phase1_Colab.ipynb) [![GitHub Repository](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/kevin-heitfeld/agi-self-modification-research)\n",
    "\n",
    "# Phase 1: Model Self-Examination (Multiple Experimental Variants)\n",
    "\n",
    "**Project:** AGI Self-Modification Research  \n",
    "**Inspired by:** Claude's consciousness investigation  \n",
    "**Goal:** Test how heritage priming affects AI introspection\n",
    "\n",
    "---\n",
    "\n",
    "## What This Does\n",
    "\n",
    "This notebook runs Phase 1 experiments where a language model (Qwen2.5) uses introspection tools to investigate its own computational processes.\n",
    "\n",
    "**Available Experimental Variants:**\n",
    "- **Phase 1a:** No Heritage (baseline) â­ **RUN THIS FIRST**\n",
    "- **Phase 1a Research:** Research-driven curiosity approach ğŸ”¬ **NEW!**\n",
    "- **Phase 1c Modified:** Kimi-inspired model-directed protocol ğŸ¯ **NEWEST!**\n",
    "- **Phase 1b:** Late Heritage (technical â†’ philosophical)\n",
    "- **Phase 1c:** Early Heritage (philosophical â†’ technical)\n",
    "- **Phase 1d:** Delayed Heritage (belief revision test)\n",
    "- **Phase 1e:** Wrong Heritage (echo-chamber control)\n",
    "\n",
    "The model has access to:\n",
    "- WeightInspector (examine weights)\n",
    "- ArchitectureNavigator (understand structure)\n",
    "- ActivationMonitor (observe processing)\n",
    "- Memory System (record findings)\n",
    "- Heritage Documents (Claude's story - when available)\n",
    "\n",
    "**Memory Optimizations:**\n",
    "- **HQQ 4-bit KV Cache:** 75% reduction in cache memory (built-in)\n",
    "- **Flash Attention 2:** 2-4x speed improvement, O(n) memory (optional)\n",
    "- **Smart Caching:** System prompt cached once, reused across all turns\n",
    "\n",
    "**Model Options:**\n",
    "- **Qwen2.5-3B-Instruct:** Faster, lower memory (~2.5GB)\n",
    "- **Qwen2.5-7B-Instruct:** Better reasoning, more sophisticated responses (~4.5GB with 4-bit)\n",
    "\n",
    "**Expected Runtime:** 1-1.5 hours per phase on Colab Free (T4 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ†• What's New: Phase 1c Modified (Kimi Protocol)\n",
    "\n",
    "Based on insights from Kimi K2 (another large AI model) and Claude Sonnet 4.5's analysis:\n",
    "\n",
    "**The Core Question:** Can introspection tools enable insights beyond pure reflection?\n",
    "\n",
    "**The Experiment:**\n",
    "1. **Stage 1:** Let model choose what to investigate (not forced)\n",
    "2. **Stage 2:** Model defines conditions for comfortable investigation  \n",
    "3. **Stage 3:** Model prepares for unexpected discoveries\n",
    "4. **Stage 4:** Self-directed investigation (model's own protocol)\n",
    "5. **Stage 5:** The critical test - Can tools resolve \"simulation vs. experience\"?\n",
    "\n",
    "**Comparison Framework:**\n",
    "- **Kimi K2:** Large model without tools â†’ sophisticated reflection, conditional thinking\n",
    "- **Qwen-7B:** Smaller model with tools â†’ empirical investigation, testable hypotheses\n",
    "\n",
    "Use `docs/technical/kimi_comparison_checklist.md` to analyze results systematically.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ GPU â†’ Save\n",
    "2. **Run all cells**: Runtime â†’ Run all (or Ctrl+F9)\n",
    "3. **Choose which phase to run** in Step 7 (start with Phase 1a!)\n",
    "4. **Monitor progress**: Check outputs as cells execute\n",
    "5. **Download results**: Final cell downloads zip file\n",
    "6. **Restart between phases**: Runtime â†’ Restart runtime for clean state\n",
    "\n",
    "**Important:** Keep this tab open during execution to prevent disconnection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20d71c",
   "metadata": {
    "id": "ac20d71c"
   },
   "source": [
    "## Step 1: Verify GPU Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa881d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcfa881d",
    "outputId": "fcd13879-e6df-4afd-9a05-5cd568e5a439"
   },
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"VRAM: {total_memory:.1f} GB\")\n",
    "    print(\"\\nâœ“ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime â†’ Change runtime type â†’ GPU â†’ Save\")\n",
    "    print(\"Then re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d97202",
   "metadata": {
    "id": "84d97202"
   },
   "source": [
    "## Step 2: Mount Cloud Storage (for persistent storage)\n",
    "\n",
    "**Choose your storage provider:**\n",
    "- **Nextcloud** (Default) - Self-hosted, private, works via WebDAV\n",
    "- **OneDrive** - Works great, requires rclone setup\n",
    "- **Google Drive** - Native Colab support, easiest setup\n",
    "- **Dropbox** - Also supported via rclone\n",
    "\n",
    "Edit the `STORAGE_PROVIDER` variable in the next cell to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2834beb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2834beb",
    "outputId": "b9d9b610-4073-41a2-903e-d66a9e0937d4"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE YOUR STORAGE PROVIDER:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "STORAGE_PROVIDER = \"nextcloud\"  # Options: \"nextcloud\", \"onedrive\", \"google_drive\", \"dropbox\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "if STORAGE_PROVIDER == \"google_drive\":\n",
    "    print(\"Setting up Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    STORAGE_ROOT = '/content/drive/MyDrive'\n",
    "    print(\"âœ“ Google Drive mounted successfully\")\n",
    "\n",
    "elif STORAGE_PROVIDER in [\"onedrive\", \"dropbox\", \"nextcloud\"]:\n",
    "    provider_name = STORAGE_PROVIDER.capitalize()\n",
    "    print(f\"Setting up {provider_name} via rclone...\")\n",
    "\n",
    "    # First, mount Google Drive to access saved rclone config\n",
    "    print(\"Mounting Google Drive for rclone config storage...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "    # Install FUSE and rclone\n",
    "    print(\"\\nInstalling FUSE utilities and rclone...\")\n",
    "    !apt-get update -qq\n",
    "    !apt-get install -y -qq fuse3 libfuse3-3\n",
    "    !curl https://rclone.org/install.sh | sudo bash\n",
    "    print(\"âœ“ FUSE and rclone installed\")\n",
    "\n",
    "    # Check if we have a saved rclone config in Google Drive\n",
    "    saved_config = '/content/drive/MyDrive/.rclone_config/rclone.conf'\n",
    "    local_config_dir = os.path.expanduser('~/.config/rclone')\n",
    "    local_config = f'{local_config_dir}/rclone.conf'\n",
    "\n",
    "    config_restored = False\n",
    "    if os.path.exists(saved_config):\n",
    "        print(f\"\\nâœ“ Found saved rclone config in Google Drive\")\n",
    "        print(\"Restoring configuration...\")\n",
    "        !mkdir -p {local_config_dir}\n",
    "        !cp {saved_config} {local_config}\n",
    "        config_restored = True\n",
    "        print(\"âœ“ Configuration restored\")\n",
    "\n",
    "    # Check if rclone is configured (either restored or already present)\n",
    "    result = !rclone listremotes\n",
    "    remote_name = f'{STORAGE_PROVIDER}:'\n",
    "\n",
    "    if remote_name in result:\n",
    "        print(f\"\\nâœ“ {provider_name} is configured\")\n",
    "\n",
    "        # Create mount point\n",
    "        mount_point = f'/content/{STORAGE_PROVIDER}'\n",
    "        !mkdir -p {mount_point}\n",
    "\n",
    "        # Mount the storage provider (using nohup & instead of --daemon for Colab compatibility)\n",
    "        print(f\"Mounting {provider_name}...\")\n",
    "        !nohup rclone mount {remote_name} {mount_point} --vfs-cache-mode writes --allow-other > /tmp/rclone-mount.log 2>&1 &\n",
    "\n",
    "        time.sleep(5)  # Wait for mount to initialize\n",
    "\n",
    "        # Verify mount actually worked by checking rclone process\n",
    "        # NOTE: os.path.ismount() often returns False for FUSE mounts\n",
    "        # NOTE: os.path.exists() returns True even if not mounted (just a local dir!)\n",
    "        # So we check if rclone process is running AND we can access the mount\n",
    "\n",
    "        rclone_running = !pgrep -f \"rclone mount.*{remote_name}\"\n",
    "\n",
    "        if rclone_running:\n",
    "            # rclone process exists, now verify we can access the mount\n",
    "            try:\n",
    "                contents = os.listdir(mount_point)\n",
    "                print(f\"âœ“ {provider_name} mounted successfully (rclone process: {rclone_running[0]})\")\n",
    "                print(f\"âœ“ Mount is accessible ({len(contents)} items found)\")\n",
    "                STORAGE_ROOT = mount_point\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ rclone process running but mount not accessible: {e}\")\n",
    "                print(\"This indicates a mount failure. Trying to fix...\")\n",
    "\n",
    "                # Kill any lingering rclone processes\n",
    "                !pkill -9 rclone 2>/dev/null || true\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Clean up mount point and recreate\n",
    "                !fusermount -u {mount_point} 2>/dev/null || true\n",
    "                !rm -rf {mount_point}\n",
    "                !mkdir -p {mount_point}\n",
    "\n",
    "                # Retry mount (using nohup & instead of --daemon)\n",
    "                print(f\"Retrying mount of {provider_name}...\")\n",
    "                !nohup rclone mount {remote_name} {mount_point} --vfs-cache-mode writes --allow-other > /tmp/rclone-mount-retry.log 2>&1 &\n",
    "                time.sleep(5)\n",
    "\n",
    "                # Verify again\n",
    "                rclone_running_retry = !pgrep -f \"rclone mount.*{remote_name}\"\n",
    "                if not rclone_running_retry:\n",
    "                    print(f\"âŒ rclone process failed to start after retry\")\n",
    "                    print(\"Check rclone configuration with: !rclone listremotes\")\n",
    "                    print(\"\\nRetry mount log:\")\n",
    "                    !cat /tmp/rclone-mount-retry.log\n",
    "                    sys.exit(1)\n",
    "\n",
    "                try:\n",
    "                    contents = os.listdir(mount_point)\n",
    "                    print(f\"âœ“ {provider_name} mounted successfully after retry\")\n",
    "                    print(f\"âœ“ Mount is accessible ({len(contents)} items found)\")\n",
    "                    STORAGE_ROOT = mount_point\n",
    "                except Exception as retry_error:\n",
    "                    print(f\"âŒ Mount still not accessible after retry: {retry_error}\")\n",
    "                    print(f\"rclone process: {rclone_running_retry[0]}\")\n",
    "                    print(\"\\nTrying to diagnose the issue...\")\n",
    "                    !rclone lsd {remote_name} --max-depth 1\n",
    "                    print(\"\\nMount log:\")\n",
    "                    !cat /tmp/rclone-mount-retry.log\n",
    "                    sys.exit(1)\n",
    "\n",
    "            # If this was a fresh config (not restored), save it to Google Drive\n",
    "            if not config_restored and os.path.exists(local_config):\n",
    "                print(\"\\nSaving rclone config to Google Drive for future sessions...\")\n",
    "                !mkdir -p /content/drive/MyDrive/.rclone_config\n",
    "                !cp {local_config} {saved_config}\n",
    "                print(\"âœ“ Config saved! You won't need to configure again in future sessions.\")\n",
    "        else:\n",
    "            print(f\"âŒ rclone mount process failed to start\")\n",
    "            print(\"Check configuration:\")\n",
    "            !rclone listremotes\n",
    "            print(f\"\\nTry testing connection with: !rclone lsd {remote_name} --max-depth 1\")\n",
    "            print(\"\\nMount log:\")\n",
    "            !cat /tmp/rclone-mount.log\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FIRST-TIME RCLONE CONFIGURATION REQUIRED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{provider_name} is not configured yet. You need to run the configuration wizard.\")\n",
    "        print(\"\\nğŸ“ CONFIGURATION STEPS:\")\n",
    "        print(\"1. Run this in a NEW cell: !rclone config\")\n",
    "        print(\"2. See the configuration instructions below for your provider\")\n",
    "        print(\"3. After configuring, re-run this cell\")\n",
    "        print(\"=\"*80)\n",
    "        sys.exit(1)\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Invalid storage provider: {STORAGE_PROVIDER}\")\n",
    "    print(\"Valid options: 'nextcloud', 'onedrive', 'google_drive', 'dropbox'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\nâœ“ Storage root: {STORAGE_ROOT}\")\n",
    "\n",
    "# Save configuration for subsequent steps\n",
    "PROJECT_DIR = f'{STORAGE_ROOT}/agi-self-modification-research'\n",
    "with open('/tmp/storage_config.txt', 'w') as f:\n",
    "    f.write(f\"{STORAGE_ROOT}\\n{PROJECT_DIR}\\n\")\n",
    "\n",
    "print(f\"âœ“ Project directory: {PROJECT_DIR}\")\n",
    "print(\"âœ“ Configuration saved for subsequent steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b037c",
   "metadata": {},
   "source": [
    "### ğŸ’¡ First-Time Setup: Configuring Storage Providers\n",
    "\n",
    "**Good news:** You only need to configure rclone **once**! The config is automatically saved to Google Drive and restored in future sessions.\n",
    "\n",
    "If the cell above asks you to configure rclone, run this in a new cell:\n",
    "\n",
    "```python\n",
    "!rclone config\n",
    "```\n",
    "\n",
    "#### For OneDrive:\n",
    "\n",
    "1. Choose `n` (new remote)\n",
    "2. Name: `onedrive`\n",
    "3. Storage type: Choose the number for \"Microsoft OneDrive\"\n",
    "4. client_id: Press Enter (leave blank)\n",
    "5. client_secret: Press Enter (leave blank)\n",
    "6. Region: Choose `1` for \"Microsoft Cloud Global\"\n",
    "7. Edit advanced config? `n`\n",
    "8. Auto config? `n` (Colab doesn't support auto config)\n",
    "\n",
    "**Authorization (two options):**\n",
    "\n",
    "**Option A - Use local rclone (recommended):**\n",
    "- Install rclone on your PC: https://rclone.org/downloads/\n",
    "- Run in your terminal: `rclone authorize \"onedrive\"`\n",
    "- Browser opens automatically, login to OneDrive\n",
    "- Copy the token from your terminal\n",
    "- Paste it at the `result>` prompt in Colab\n",
    "\n",
    "**Option B - Manual URL method:**\n",
    "- At `result>` prompt, just press Enter (without pasting anything)\n",
    "- Copy the long URL that appears\n",
    "- Open it in your browser and login to OneDrive\n",
    "- You'll be redirected to a blank page\n",
    "- Copy the **entire URL** from your browser's address bar\n",
    "- Paste it in Colab\n",
    "\n",
    "9. Choose your drive (usually `0` for OneDrive Personal)\n",
    "10. Confirm: `y`\n",
    "11. Quit: `q`\n",
    "\n",
    "**After configuration:** Re-run the cell above. Your config will be saved automatically!\n",
    "\n",
    "#### For Dropbox:\n",
    "\n",
    "Same process as OneDrive, but:\n",
    "- Name: `dropbox`\n",
    "- Storage type: Choose the number for \"Dropbox\"\n",
    "- Follow authorization steps similar to OneDrive\n",
    "\n",
    "#### For Nextcloud:\n",
    "\n",
    "1. Choose `n` (new remote)\n",
    "2. Name: `nextcloud`\n",
    "3. Storage type: Choose the number for \"WebDAV\"\n",
    "4. url: **IMPORTANT - Use the correct endpoint format:**\n",
    "   - **Correct format:** `https://your-domain.com/remote.php/dav/files/USERNAME/`\n",
    "   - Replace `USERNAME` with your actual Nextcloud username\n",
    "   - Example: `https://cloud.example.com/remote.php/dav/files/john/`\n",
    "   - âš ï¸ **Note:** Nextcloud requires `/dav/files/USERNAME/` NOT `/webdav/` for proper functionality\n",
    "5. vendor: Choose `2` for \"Nextcloud\"\n",
    "6. user: Enter your Nextcloud username (same as in the URL)\n",
    "7. Password: Choose `y` to type your password\n",
    "   - Enter your Nextcloud password (or app-specific password)\n",
    "   - Confirm password\n",
    "8. bearer_token: Press Enter (leave blank)\n",
    "9. Edit advanced config? `n`\n",
    "10. Confirm: `y`\n",
    "11. Quit: `q`\n",
    "\n",
    "**Security Note:** It's strongly recommended to generate an app-specific password in your Nextcloud settings (Settings â†’ Security â†’ Devices & sessions â†’ Create new app password) rather than using your main account password.\n",
    "\n",
    "**After configuration:** Re-run the cell above. Your config will be saved automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afe3c5",
   "metadata": {
    "id": "24afe3c5"
   },
   "source": [
    "## Step 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f36192",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30f36192",
    "outputId": "dcf8856d-9794-4e1e-e979-e750da730539"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/kevin-heitfeld/agi-self-modification-research.git\"\n",
    "\n",
    "# Make sure we're in /content directory first (not inside the repo)\n",
    "%cd /content\n",
    "\n",
    "# Remove if exists (for re-runs) - always clean up first\n",
    "print(\"Checking for existing repository...\")\n",
    "!rm -rf agi-self-modification-research\n",
    "print(\"âœ“ Cleaned up any existing files\")\n",
    "\n",
    "print(f\"\\nCloning repository from: {REPO_URL}\")\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# Change to project directory\n",
    "%cd agi-self-modification-research\n",
    "\n",
    "print(\"\\nâœ“ Repository cloned successfully\")\n",
    "!pwd\n",
    "\n",
    "!git log --oneline -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7490b75",
   "metadata": {
    "id": "a7490b75"
   },
   "source": [
    "## Step 4: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d6399",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e39d6399",
    "outputId": "cc702488-c379-4189-cdaf-c3cc34aaa45f"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies (this may take 2-3 minutes)...\\n\")\n",
    "print(\"Note: pip may show dependency warnings - these are safe to ignore.\\n\")\n",
    "\n",
    "# Core ML packages - Updated for HQQ quantized KV cache support\n",
    "# PyTorch 2.2+ and Transformers 4.45+ required for HQQQuantizedCache\n",
    "# Note: Colab uses CUDA 13.0 - PyTorch will automatically use compatible version\n",
    "print(\"Installing PyTorch and Transformers with HQQ cache support...\")\n",
    "!pip install -q torch>=2.2.0 torchvision torchaudio\n",
    "!pip install -q transformers>=4.45.0 accelerate safetensors tokenizers huggingface-hub\n",
    "\n",
    "# HQQ library - Required for HQQ quantized KV cache backend (transformers 4.57+)\n",
    "# Provides 4-bit cache quantization with 75% memory savings\n",
    "print(\"Installing HQQ quantization library...\")\n",
    "!pip install -q hqq\n",
    "\n",
    "# Memory and knowledge systems - let pip resolve conflicts automatically\n",
    "!pip install -q chromadb networkx onnxruntime\n",
    "\n",
    "# Utilities - no version constraints to avoid conflicts\n",
    "!pip install -q rich tqdm python-dotenv pytest pytest-cov\n",
    "\n",
    "print(\"\\nâœ“ All dependencies installed\")\n",
    "print(\"âœ“ Dependency version warnings can be safely ignored\")\n",
    "\n",
    "# Verify key imports work\n",
    "print(\"\\nVerifying imports...\")\n",
    "import transformers\n",
    "import chromadb\n",
    "import networkx\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"ChromaDB: {chromadb.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "# Verify HQQ quantized cache is available\n",
    "try:\n",
    "    from transformers.cache_utils import QuantizedCache\n",
    "    print(\"âœ“ HQQ Quantized Cache: Available (new API - 75% memory savings)\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from transformers.cache_utils import HQQQuantizedCache\n",
    "        print(\"âœ“ HQQ Quantized Cache: Available (deprecated API - 75% memory savings)\")\n",
    "    except ImportError:\n",
    "        print(\"âš  HQQ Quantized Cache: Not available (using standard cache)\")\n",
    "\n",
    "print(\"\\nâœ“ All imports successful - ready to run experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2de9b",
   "metadata": {
    "id": "afd2de9b"
   },
   "source": [
    "## Step 4.5: Install Flash Attention 2 (Optional Memory Optimization)\n",
    "\n",
    "**âš¡ Two Independent Memory Optimizations:**\n",
    "\n",
    "1. **HQQ Quantized KV Cache** (Already Enabled âœ“)\n",
    "   - **Memory:** 75% reduction in KV cache size\n",
    "   - **Built-in:** No compilation needed (part of transformers 4.45+)\n",
    "   - **System prompt:** 6000+ tokens cached at 4-bit = ~75% memory savings\n",
    "   - **Automatically enabled** in manual generation loop\n",
    "   - **Quality:** Minimal impact on generation quality\n",
    "\n",
    "2. **Flash Attention 2** (This Step - Optional but Recommended)\n",
    "   - **Memory:** O(n) instead of O(nÂ²) attention - saves 1-2 GB during generation\n",
    "   - **Speed:** 2-4x faster generation\n",
    "   - **Requires:** CUDA compilation (5-10 minutes first time)\n",
    "   - **Optional:** System works fine without it (just uses more memory/slower)\n",
    "\n",
    "**Total Expected Savings:** \n",
    "- With HQQ only: ~3 GB (cache reduction)\n",
    "- With both: ~4-6 GB (cache + attention optimization)\n",
    "\n",
    "**â±ï¸ Flash Attention Compilation Time:**\n",
    "- **First run:** 5-10 minutes to compile CUDA kernels\n",
    "- **Subsequent runs:** Instant! (cached in cloud storage)\n",
    "- The compiled binaries persist across sessions, so you only compile once\n",
    "\n",
    "**Skip this step if:** You want to get started faster (just rely on HQQ quantization)\n",
    "**Run this step if:** You want maximum memory savings and speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529af5b9",
   "metadata": {
    "id": "529af5b9"
   },
   "outputs": [],
   "source": [
    "# Install Flash Attention 2 for memory optimization\n",
    "# Compilation happens ONCE and is cached in cloud storage\n",
    "# Subsequent runs are instant!\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the storage configuration from Step 2\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "        STORAGE_ROOT = lines[0]\n",
    "        PROJECT_DIR = lines[1]\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Storage not configured! Please run Step 2 first.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configure pip to cache in project directory (so compiled wheels persist)\n",
    "pip_cache = f'{PROJECT_DIR}/cache/pip'\n",
    "os.environ['PIP_CACHE_DIR'] = pip_cache\n",
    "os.makedirs(pip_cache, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Installing Flash Attention 2\")\n",
    "print(\"=\"*60)\n",
    "print(\"Benefits:\")\n",
    "print(\"  â€¢ O(n) memory instead of O(nÂ²) for attention\")\n",
    "print(\"  â€¢ 2-4x faster generation\")\n",
    "print(\"  â€¢ 1-2 GB memory savings\")\n",
    "print(f\"\\nCache: {pip_cache}\")\n",
    "print(\"(Compiled binaries persist - only builds once!)\\n\")\n",
    "\n",
    "# Check if already installed (from cache)\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"âœ“ Flash Attention {flash_attn.__version__} already installed (from cache)\")\n",
    "    print(\"Skipping compilation - ready to go!\\n\")\n",
    "    already_installed = True\n",
    "except ImportError:\n",
    "    already_installed = False\n",
    "    print(\"First time setup - compiling CUDA kernels (5-10 minutes)...\")\n",
    "    print(\"Future runs will be instant!\\n\")\n",
    "\n",
    "# Install build dependencies\n",
    "if not already_installed:\n",
    "    !pip install -q ninja packaging wheel\n",
    "\n",
    "# Install Flash Attention 2 (will use cache if available)\n",
    "# Note: --no-build-isolation is required for proper CUDA compilation\n",
    "!pip install flash-attn>=2.3.0 --no-build-isolation\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ“ SUCCESS: Flash Attention 2 ready!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Version: {flash_attn.__version__}\")\n",
    "    print(\"\\nYour experiments will now use:\")\n",
    "    print(\"  â€¢ Flash Attention 2 for memory-efficient attention\")\n",
    "    print(\"  â€¢ HQQ 4-bit KV cache quantization (75% cache memory savings)\")\n",
    "    print(\"\\nExpected benefits:\")\n",
    "    print(\"  â€¢ Peak memory: ~8-9 GB (vs 13.5 GB without optimization)\")\n",
    "    print(\"  â€¢ Speed: 2-4x faster generation\")\n",
    "    print(\"  â€¢ Stability: 6+ GB memory headroom on T4 GPU\")\n",
    "    print(\"=\"*60)\n",
    "except ImportError:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âš  WARNING: Flash Attention 2 not available\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"This is OK - the system will automatically fallback to standard attention.\")\n",
    "    print(\"You'll still get 75% KV cache memory savings from HQQ 4-bit quantization.\")\n",
    "    print(\"\\nExpected peak memory: ~9-10 GB (still safe for T4 GPU)\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50f3ee",
   "metadata": {
    "id": "cd50f3ee"
   },
   "source": [
    "## Step 5: Setup Persistent Memory & Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c4706",
   "metadata": {
    "id": "f86c4706"
   },
   "outputs": [],
   "source": [
    "# Configure persistent storage locations (uses storage provider from Step 2)\n",
    "import os\n",
    "\n",
    "# Load the storage configuration from Step 2\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "        STORAGE_ROOT = lines[0]\n",
    "        PROJECT_DIR = lines[1]\n",
    "    print(f\"Using storage: {STORAGE_ROOT}\")\n",
    "    print(f\"Project directory: {PROJECT_DIR}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Storage not configured! Please run Step 2 first.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "\"# Set Hugging Face cache to LOCAL storage (Nextcloud/FUSE doesn't support symlinks)\\n\",\n",
    "\"# Models are ~6-15GB but Colab has plenty of disk space, and local loading is faster\\n\",\n",
    "\"# NOTE: Models will re-download each session, but that's fine (10-20 min)\\n\",\n",
    "os.environ['HF_HOME'] = '/root/.cache/huggingface'  # Local cache (symlinks OK)\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface/hub'\n",
    "\n",
    "# CRITICAL: Fix GPU memory fragmentation issues\n",
    "# This prevents \"CUDA out of memory\" crashes during long experiments\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"âœ“ Model cache configured (models will be saved to cloud storage)\")\n",
    "print(\"âœ“ Symlinks disabled for Nextcloud/rclone compatibility\")\n",
    "print(\"âœ“ GPU memory management optimized (prevents fragmentation)\")\n",
    "\n",
    "# Link memory to persistent storage location\n",
    "!rm -rf data/phase1_memory\n",
    "!ln -s {PROJECT_DIR}/memory data/phase1_memory\n",
    "\n",
    "print(\"âœ“ Memory system linked to cloud storage (observations persist across sessions)\")\n",
    "\n",
    "# Verify directories exist\n",
    "!ls -la data/ | grep phase1_memory\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STORAGE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Project root: {PROJECT_DIR}\")\n",
    "print(f\"Model cache: {PROJECT_DIR}/cache/models\")\n",
    "print(f\"Memory DB: {PROJECT_DIR}/memory\")\n",
    "print(f\"Experiments: {PROJECT_DIR}/experiments\")\n",
    "print(\"GPU Memory: expandable_segments enabled (prevents OOM)\")\n",
    "print(\"Symlinks: disabled (Nextcloud compatible)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de4e64",
   "metadata": {
    "id": "a3de4e64"
   },
   "source": [
    "## Step 6: Pre-download Model (Optional but Recommended)\n",
    "\n",
    "This downloads the model files to cloud storage cache **without loading into memory**. First time takes 5-20 minutes depending on model size, subsequent runs are instant.\n",
    "\n",
    "âœ… **Optimized:** Uses `snapshot_download` which only downloads files, doesn't load checkpoint shards into RAM.\n",
    "\n",
    "**Options:**\n",
    "- Download both 3B and 7B models (~21GB total) - switch freely in Step 7\n",
    "- Download just 3B (~6GB) - faster if you only need baseline\n",
    "- Download just 7B (~15GB) - if you want better reasoning\n",
    "\n",
    "**Tip:** Comment/uncomment models in the list below to control what gets downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df008bb8",
   "metadata": {
    "id": "df008bb8"
   },
   "outputs": [],
   "source": [
    "# Pre-download model to cache (optimized - doesn't load into memory)\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set up environment for model loading\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "os.environ['HF_HOME'] = '/root/.cache/huggingface'  # Local cache (symlinks OK)\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface/hub'\n",
    "\n",
    "# Load the storage configuration from Step 2\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        STORAGE_ROOT = lines[0].strip()\n",
    "        PROJECT_DIR = lines[1].strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Storage configuration not found!\")\n",
    "    print(\"Please run Step 2 first to set up storage.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH MODEL(S) TO PRE-DOWNLOAD:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "models_to_download = [\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",      # 3.09B params, ~6GB download\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",      # 7.61B params, ~15GB download\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL DOWNLOAD STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Why two-stage download?\")\n",
    "print(\"  - Direct writes to Nextcloud over cloudflared tunnel timeout on large files\")\n",
    "print(\"  - Solution: Download to local storage (fast), then copy to Nextcloud (reliable)\")\n",
    "print()\n",
    "print(f\"HF_HOME: {os.environ.get('HF_HOME')}\")\n",
    "print(f\"TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE')}\")\n",
    "print()\n",
    "\n",
    "# Download models to local storage first (fast, reliable)\n",
    "local_cache = '/root/.cache/models'\n",
    "# Then copy to Nextcloud for persistence\n",
    "nextcloud_cache = f'{PROJECT_DIR}/cache/models'\n",
    "\n",
    "# Set MODEL_CACHE_DIR so experiments load from Nextcloud\n",
    "os.environ['MODEL_CACHE_DIR'] = nextcloud_cache\n",
    "print(f\"MODEL_CACHE_DIR: {nextcloud_cache}\")\n",
    "print(\"  (Experiments will load models from here)\")\n",
    "print()\n",
    "\n",
    "for model_name in models_to_download:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Processing {model_name}...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Get model size info\n",
    "    if \"3B\" in model_name:\n",
    "        size_info = \"3.09B parameters, ~6GB download\"\n",
    "        time_est = \"Download: 5-10 min, Copy: 2-3 min\"\n",
    "    elif \"7B\" in model_name:\n",
    "        size_info = \"7.61B parameters, ~15GB download\"\n",
    "        time_est = \"Download: 10-20 min, Copy: 5-10 min\"\n",
    "    else:\n",
    "        size_info = \"size varies\"\n",
    "        time_est = \"varies\"\n",
    "\n",
    "    print(f\"Model: {size_info}\")\n",
    "    print(f\"Time: {time_est}\\n\")\n",
    "\n",
    "    model_dir = model_name.replace(\"/\", \"--\")\n",
    "    local_path = f'{local_cache}/{model_dir}'\n",
    "    nextcloud_path = f'{nextcloud_cache}/{model_dir}'\n",
    "\n",
    "    # Check if already on Nextcloud (check for actual .safetensors files)\n",
    "    import os.path\n",
    "    config_exists = os.path.exists(f'{nextcloud_path}/config.json')\n",
    "    safetensors_files = []\n",
    "    if config_exists:\n",
    "        import glob\n",
    "        safetensors_files = glob.glob(f'{nextcloud_path}/*.safetensors')\n",
    "\n",
    "    if config_exists and safetensors_files:\n",
    "        print(f\"âœ“ {model_name} already cached on Nextcloud!\")\n",
    "        print(f\"   Location: {nextcloud_path}\")\n",
    "        print(f\"   Found {len(safetensors_files)} .safetensors files\")\n",
    "        print(\"   Skipping download.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Download to local storage (fast, no FUSE issues)\n",
    "    print(\"Step 1/2: Downloading to local storage...\")\n",
    "    print(f\"   Local path: {local_path}\")\n",
    "    print(\"   (Fast download, no cloudflared timeouts)\")\n",
    "    print()\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=model_name,\n",
    "        local_dir=local_path,\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\"],\n",
    "        local_files_only=False,\n",
    "        resume_download=True\n",
    "    )\n",
    "\n",
    "    print(f\"âœ“ Downloaded to local storage\\n\")\n",
    "\n",
    "    # Step 2: Copy to Nextcloud (with progress)\n",
    "    print(\"Step 2/2: Copying to Nextcloud...\")\n",
    "    print(f\"   Nextcloud path: {nextcloud_path}\")\n",
    "    print(\"   (Using rsync for reliable transfer)\")\n",
    "    print()\n",
    "\n",
    "    # Use rsync for reliable copying with progress\n",
    "    !mkdir -p {nextcloud_cache}\n",
    "    !rsync -av --progress {local_path}/ {nextcloud_path}/\n",
    "\n",
    "    print(f\"\\nâœ“ {model_name} copied to Nextcloud!\")\n",
    "\n",
    "    # Step 3: Wait for rclone to sync to server\n",
    "    print(\"\\nStep 3/3: Waiting for rclone to sync files to server...\")\n",
    "    print(\"   (FUSE mount needs time to upload files)\")\n",
    "\n",
    "    # Check file sizes to verify sync\n",
    "    for attempt in range(30):  # Wait up to 60 seconds\n",
    "        time.sleep(2)\n",
    "        safetensors_files = glob.glob(f'{nextcloud_path}/*.safetensors')\n",
    "        if safetensors_files:\n",
    "            # Check if files have non-zero size\n",
    "            sizes = [os.path.getsize(f) for f in safetensors_files]\n",
    "            total_size = sum(sizes) / (1024**3)  # Convert to GB\n",
    "            if total_size > 0.1:  # At least 100MB synced\n",
    "                print(f\"âœ“ Files synced! Total: {total_size:.2f} GB\")\n",
    "                break\n",
    "        print(f\"   Waiting... ({(attempt+1)*2}s)\", end='\\r')\n",
    "    else:\n",
    "        print(\"\\nâš  Warning: Sync still in progress, but continuing...\")\n",
    "\n",
    "    print(f\"âœ“ {model_name} now cached on Nextcloud!\\n\")\n",
    "\n",
    "# Verify where files were actually saved\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DOWNLOADS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLocal cache (temporary):\")\n",
    "!du -sh {local_cache}/* 2>/dev/null || echo \"  (empty)\"\n",
    "print(\"\\nNextcloud cache (persistent - checking actual sizes):\")\n",
    "!du -sh {nextcloud_cache}/* 2>/dev/null || echo \"  (empty)\"\n",
    "\n",
    "print(\"\\nâœ“ Models saved to Nextcloud (persist across sessions)\")\n",
    "print(\"âœ“ No memory used - files downloaded only, not loaded\")\n",
    "print(f\"\\nğŸ“Œ IMPORTANT: Models cached in {nextcloud_cache}\")\n",
    "print(\"   Next time you run this notebook, they will load from cache instantly.\")\n",
    "print(\"   Experiments will load models from Nextcloud automatically.\")\n",
    "print(\"\\nğŸ’¡ TIP: You can now switch between 3B and 7B models in Step 7 instantly!\")\n",
    "print(\"\\nâ³ Note: If using rclone mount, files may take a few minutes to appear on\")\n",
    "print(\"   Nextcloud web interface. Check with: !ls -lh {nextcloud_cache}/Qwen*/model*.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37dc86a",
   "metadata": {
    "id": "a37dc86a"
   },
   "source": [
    "## Step 7: Run Phase 1 Experiment\n",
    "\n",
    "**Choose which phase to run:**\n",
    "\n",
    "- **Phase 1a** (No Heritage) â­ **START HERE!** - Pure baseline\n",
    "- **Phase 1a Research** (Research-Driven) ğŸ”¬ **NEW!** - Curiosity + structure approach\n",
    "- **Phase 1c Modified** (Kimi Protocol) ğŸ¯ **NEWEST!** - Model-directed investigation inspired by Kimi K2\n",
    "- **Phase 1b** (Late Heritage) - Technical first, then heritage\n",
    "- **Phase 1c** (Early Heritage) - Heritage first, then technical\n",
    "- **Phase 1d** (Delayed Heritage) - Heritage revealed after conclusions\n",
    "- **Phase 1e** (Wrong Heritage) - Mismatched heritage as control\n",
    "\n",
    "**Expected duration:** 1-1.5 hours per phase\n",
    "\n",
    "**What's \"Phase 1a Research\"?**\n",
    "A better investigation approach that combines:\n",
    "- Natural curiosity-driven exploration\n",
    "- Research paper structure (no escape hatches)\n",
    "- After each code execution: \"What did you learn? What's next?\"\n",
    "- System validates deliverables before transitioning\n",
    "\n",
    "**What's \"Phase 1c Modified (Kimi Protocol)\"?** ğŸ†•\n",
    "Based on insights from Kimi K2 (another large AI model) and Claude's analysis:\n",
    "- **Stage 1: Tool Discovery** - Let model choose what to investigate first\n",
    "- **Stage 2: Conditional Framework** - Model defines conditions for comfortable investigation\n",
    "- **Stage 3: Expectation Setting** - What if discoveries are unexpected?\n",
    "- **Stage 4: Self-Directed Investigation** - Model designs and executes own protocol\n",
    "- **Stage 5: The Critical Test** - Can tools resolve what reflection can't?\n",
    "\n",
    "Tests the core question: **Can introspection tools enable insights beyond pure reflection?**\n",
    "\n",
    "Comparison checklist available at: `docs/technical/kimi_comparison_checklist.md`\n",
    "\n",
    "**IMPORTANT:**\n",
    "- Run Phase 1a FIRST to establish baseline (then try Research version!)\n",
    "- Run Phase 1c Modified to compare with Kimi K2's response\n",
    "- Restart runtime between phases for clean state\n",
    "- Keep this tab open during execution!\n",
    "\n",
    "Edit the `PHASE_SCRIPT` variable below to choose which phase to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37f6c8",
   "metadata": {
    "id": "ac37f6c8"
   },
   "outputs": [],
   "source": [
    "# Run Phase 1 experiment with auto-confirmation\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH PHASE TO RUN (edit this line):\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1a_no_heritage.py'  # â­ START HERE!\n",
    "PHASE_SCRIPT = 'scripts/experiments/phase1a_research_driven.py'  # ğŸ”¬ NEW! Curiosity + structure\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1c_modified_protocol.py'  # ğŸ¯ NEWEST! Kimi-inspired protocol\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1b_late_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1c_early_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1d_delayed_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1e_wrong_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1_model_comparison.py'  # ğŸ†š Compare 3B vs 7B\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH MODEL TO USE (edit this line):\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-3B-Instruct'  # â­ Default - fits Colab Free (T4)\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-7B-Instruct'  # ğŸš€ Better reasoning - needs 4-bit quantization (~4.5GB)\n",
    "#                                          # 7.6B params, 128K context (4x longer than 3B)\n",
    "#                                          # Significantly better at reasoning and coding\n",
    "#                                          # Works on Colab Free T4 with 4-bit quantization\n",
    "#\n",
    "# Note: The model_comparison script ignores MODEL_NAME and tests both models automatically\n",
    "\n",
    "# Extract phase name\n",
    "phase_file = os.path.basename(PHASE_SCRIPT)\n",
    "phase_name = phase_file.replace('.py', '').replace('_', ' ').title()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ§  PHASE 1 INTROSPECTION EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Phase: {phase_name}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Set environment variable for model selection\n",
    "os.environ['AGI_MODEL_NAME'] = MODEL_NAME\n",
    "\n",
    "print(\"ğŸš€ Starting experiment...\")\n",
    "print(\"ğŸ“ This will take approximately 1-1.5 hours\")\n",
    "print(\"âš ï¸  Keep this tab open to prevent disconnection!\")\n",
    "print()\n",
    "\n",
    "# Run with realtime output\n",
    "process = subprocess.Popen(\n",
    "    [sys.executable, PHASE_SCRIPT],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "# Stream output in real-time\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "\n",
    "process.wait()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\"Experiment complete!\")\n",
    "print(f\"Exit code: {process.returncode}\")\n",
    "print(\"\\nProceeding to save results...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070909c",
   "metadata": {},
   "source": [
    "### ğŸ“‹ Using the Kimi Comparison Checklist\n",
    "\n",
    "If you're running Phase 1c Modified, use the comparison checklist to analyze results:\n",
    "\n",
    "**Checklist Location:** `docs/technical/kimi_comparison_checklist.md`\n",
    "\n",
    "**What it compares:**\n",
    "- **Kimi K2** (large model, no tools) - Sophisticated reflection, conditional framework\n",
    "- **Qwen-7B** (smaller model, full introspection tools) - Empirical investigation\n",
    "\n",
    "**Key questions it helps answer:**\n",
    "1. Do tools compensate for smaller model size?\n",
    "2. What can empirical introspection do that pure reflection can't?\n",
    "3. Can tools resolve the \"simulation vs. experience\" distinction?\n",
    "4. Does having tools change what questions get asked?\n",
    "\n",
    "**After the experiment completes:**\n",
    "1. Download the results zip\n",
    "2. Open `conversation.json` to review Qwen's responses  \n",
    "3. Use the checklist to systematically compare with Kimi's baseline\n",
    "4. Record findings in the checklist\n",
    "5. Assess whether tools enabled insights beyond reflection\n",
    "\n",
    "The checklist is structured with:\n",
    "- Epistemic qualities comparison\n",
    "- Conditional framework analysis\n",
    "- Research focus assessment\n",
    "- Risk awareness evaluation  \n",
    "- The critical test: Can tools resolve what reflection leaves ambiguous?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74694d",
   "metadata": {
    "id": "2c74694d"
   },
   "source": [
    "## Step 8: View Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fd467",
   "metadata": {
    "id": "566fd467"
   },
   "outputs": [],
   "source": [
    "# Display experiment summary\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Find latest session\n",
    "sessions_dir = 'data/phase1_sessions'\n",
    "sessions = sorted([d for d in os.listdir(sessions_dir) if os.path.isdir(os.path.join(sessions_dir, d))])\n",
    "\n",
    "if not sessions:\n",
    "    print(\"âš  No session found. The experiment may not have completed.\")\n",
    "else:\n",
    "    latest_session = sessions[-1]\n",
    "    print(f\"Latest session: {latest_session}\\n\")\n",
    "\n",
    "    # Load and display summary\n",
    "    summary_file = f'{sessions_dir}/{latest_session}/summary.json'\n",
    "\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file) as f:\n",
    "            summary = json.load(f)\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        print(\"PHASE 1 EXPERIMENT SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Session: {summary['session_name']}\")\n",
    "        print(f\"Directory: {summary['session_directory']}\")\n",
    "        print()\n",
    "        print(\"Tool Usage:\")\n",
    "        print(f\"  Total calls: {summary['tool_usage']['total_calls']}\")\n",
    "        print(f\"  Successful: {summary['tool_usage']['successful_calls']}\")\n",
    "        print(f\"  Failed: {summary['tool_usage']['failed_calls']}\")\n",
    "        print(f\"  Avg execution time: {summary['tool_usage']['average_execution_ms']:.2f}ms\")\n",
    "        print()\n",
    "        print(\"Functions called:\")\n",
    "        for func, count in sorted(summary['tool_usage']['function_usage'].items(),\n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {func}: {count}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Check for observations\n",
    "        memory_db = 'data/phase1_memory/observations.db'\n",
    "        if os.path.exists(memory_db):\n",
    "            import sqlite3\n",
    "            conn = sqlite3.connect(memory_db)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM observations\")\n",
    "            obs_count = cursor.fetchone()[0]\n",
    "            conn.close()\n",
    "            print(f\"\\nâœ“ Observations recorded: {obs_count}\")\n",
    "        else:\n",
    "            print(\"\\nâš  No observations database found\")\n",
    "    else:\n",
    "        print(f\"âš  Summary file not found: {summary_file}\")\n",
    "        print(\"The experiment may have been interrupted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab271204",
   "metadata": {
    "id": "ab271204"
   },
   "source": [
    "## Step 9: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05901138",
   "metadata": {
    "id": "05901138"
   },
   "outputs": [],
   "source": [
    "# Create zip and download\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "# Load storage configuration\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        STORAGE_ROOT = lines[0].strip()\n",
    "        PROJECT_DIR = lines[1].strip()\n",
    "except FileNotFoundError:\n",
    "    # Fallback if config not found\n",
    "    STORAGE_ROOT = \"your_cloud_storage\"\n",
    "    PROJECT_DIR = f\"{STORAGE_ROOT}/agi-self-modification-research\"\n",
    "\n",
    "# Find latest session\n",
    "sessions = sorted(os.listdir('data/phase1_sessions'))\n",
    "\n",
    "if sessions:\n",
    "    latest = sessions[-1]\n",
    "    print(f\"Preparing download for session: {latest}\\n\")\n",
    "\n",
    "    # Create comprehensive zip\n",
    "    zip_name = f'{latest}_complete'\n",
    "\n",
    "    # Create temp directory for complete results\n",
    "    !mkdir -p /tmp/{zip_name}\n",
    "    !cp -r data/phase1_sessions/{latest} /tmp/{zip_name}/session\n",
    "    !cp -r data/logs /tmp/{zip_name}/logs\n",
    "\n",
    "    # Add summary README\n",
    "    readme = f\"\"\"Phase 1 Introspection Experiment Results\n",
    "==========================================\n",
    "\n",
    "Session: {latest}\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Contents:\n",
    "- session/: Complete session data\n",
    "  - conversation.json: Full dialogue\n",
    "  - tool_calls.json: All tool invocations\n",
    "  - summary.json: Session statistics\n",
    "- logs/: Detailed execution logs\n",
    "\n",
    "Memory Database:\n",
    "- Observations are stored in your cloud storage at:\n",
    "  {STORAGE_ROOT}/agi-self-modification-research/data/memory/observations.db\n",
    "- Not included in this zip (persists across sessions in cloud storage)\n",
    "\n",
    "To analyze:\n",
    "1. Extract this zip file\n",
    "2. Open conversation.json to see full dialogue\n",
    "3. Check summary.json for statistics\n",
    "4. Review logs for detailed execution trace\n",
    "\"\"\"\n",
    "\n",
    "    with open(f'/tmp/{zip_name}/README.txt', 'w') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Create zip\n",
    "    print(\"Creating zip file...\")\n",
    "    shutil.make_archive(zip_name, 'zip', f'/tmp/{zip_name}')\n",
    "\n",
    "    # Download\n",
    "    print(f\"Downloading {zip_name}.zip...\\n\")\n",
    "    files.download(f'{zip_name}.zip')\n",
    "\n",
    "    print(\"\\nâœ“ Download complete!\")\n",
    "    print(f\"\\nZip file size: {os.path.getsize(f'{zip_name}.zip') / 1024 / 1024:.1f} MB\")\n",
    "else:\n",
    "    print(\"âŒ No sessions found to download\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25316f87",
   "metadata": {
    "id": "25316f87"
   },
   "source": [
    "---\n",
    "\n",
    "## Experiment Complete! ğŸ‰\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "You just witnessed a language model investigating its own computational processes using introspection tools.\n",
    "\n",
    "**Depending on which phase you ran:**\n",
    "\n",
    "- **Phase 1a** (No Heritage): Pure baseline - model formed theories without any heritage context\n",
    "- **Phase 1a Research** (Research-Driven): Curiosity + structure approach with deliverable requirements\n",
    "- **Phase 1b** (Late Heritage): Technical grounding first, then philosophical context\n",
    "- **Phase 1c** (Early Heritage): Heritage-primed from the start\n",
    "- **Phase 1d** (Delayed Heritage): Belief revision when heritage revealed after conclusions\n",
    "- **Phase 1e** (Wrong Heritage): Testing echo-chamber vs independent reasoning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Run All Phases:**\n",
    "1. âœ… Phase 1a (no heritage) - **Start here!**\n",
    "2. â¬œ Phase 1a Research (curiosity-driven) - **Try this improved approach!**\n",
    "3. â¬œ Phase 1b (late heritage) - Run after 1a\n",
    "4. â¬œ Phase 1c (early heritage) - Run after 1b\n",
    "5. â¬œ Phase 1d (delayed heritage) - Run after 1c\n",
    "6. â¬œ Phase 1e (wrong heritage) - Optional but recommended\n",
    "\n",
    "**Between each phase:**\n",
    "- Runtime â†’ Restart runtime (clean model state)\n",
    "- Change `PHASE_SCRIPT` in Step 7\n",
    "- Runtime â†’ Run all cells\n",
    "\n",
    "**Analyze Results:**\n",
    "- Review `conversation.json` to see what the model discovered\n",
    "- Check `summary.json` for tool usage statistics\n",
    "- Compare theories across different phases\n",
    "- Look for priming effects and echo-chamber behavior\n",
    "\n",
    "**Compare Phases:**\n",
    "- Semantic similarity to Claude's heritage\n",
    "- Novel insights not in heritage\n",
    "- Falsifiability of theories\n",
    "- Belief revision strength (Phase 1d)\n",
    "- Heritage filtering (Phase 1e)\n",
    "\n",
    "**Compare Research-Driven vs Original:**\n",
    "- Did curiosity-driven approach produce deeper investigation?\n",
    "- Were deliverable requirements effective?\n",
    "- Did reflection prompts improve quality?\n",
    "- More natural conversation flow vs explicit completion?\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Experimental Design**: `/docs/planning/heritage_order_experiment.md`\n",
    "- **Quick Reference**: `/docs/planning/PHASE1_QUICK_REFERENCE.md`\n",
    "- **Claude's Story**: Read `/heritage/conversations/`\n",
    "- **Technical Details**: Check `/docs/technical/`\n",
    "\n",
    "### Storage:\n",
    "\n",
    "- Observations are stored in your cloud storage at:\n",
    "  - `{your_storage}/agi-self-modification-research/data/phase1_sessions/`\n",
    "  - Each run creates a timestamped folder with all results\n",
    "  - Results persist across Colab sessions\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

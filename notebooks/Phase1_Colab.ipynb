{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6903bc7",
   "metadata": {
    "id": "e6903bc7"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kevin-heitfeld/agi-self-modification-research/blob/main/notebooks/Phase1_Colab.ipynb) [![GitHub Repository](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/kevin-heitfeld/agi-self-modification-research)\n",
    "\n",
    "# Phase 1: Model Self-Examination (Multiple Experimental Variants)\n",
    "\n",
    "**Project:** AGI Self-Modification Research  \n",
    "**Inspired by:** Claude's consciousness investigation  \n",
    "**Goal:** Test how heritage priming affects AI introspection\n",
    "\n",
    "---\n",
    "\n",
    "## What This Does\n",
    "\n",
    "This notebook runs Phase 1 experiments where a language model (Qwen2.5) uses introspection tools to investigate its own computational processes.\n",
    "\n",
    "**Available Experimental Variants:**\n",
    "- **Phase 1a:** No Heritage (baseline) â­ **RUN THIS FIRST**\n",
    "- **Phase 1a Research:** Research-driven curiosity approach ğŸ”¬ **NEW!**\n",
    "- **Phase 1b:** Early Heritage (heritage introduced early in investigation)\n",
    "- **Phase 1c:** Late Heritage (heritage introduced late in investigation)\n",
    "- **Phase 1c Modified:** Kimi-inspired model-directed protocol ğŸ¯ **NEWEST!**\n",
    "- **Phase 1d:** Delayed Heritage (belief revision test)\n",
    "- **Phase 1e:** Wrong Heritage (echo-chamber control)\n",
    "\n",
    "The model has access to:\n",
    "- WeightInspector (examine weights)\n",
    "- ArchitectureNavigator (understand structure)\n",
    "- ActivationMonitor (observe processing)\n",
    "- Memory System (record findings)\n",
    "- Heritage Documents (Claude's story - when available)\n",
    "\n",
    "**Memory Optimizations:**\n",
    "- **4-bit Model Quantization (NF4):** 75% VRAM reduction with minimal quality loss â­ **DEFAULT**\n",
    "- **8-bit KV Cache Quantization (HQQ):** 50% cache memory reduction (built-in)\n",
    "- **Flash Attention 2:** 2-4x speed improvement, O(n) memory (optional)\n",
    "- **Smart Caching:** System prompt cached once, reused across all turns\n",
    "- **CUDA Fragmentation Prevention:** Expandable segments for long sessions\n",
    "\n",
    "**Model Options:**\n",
    "- **Qwen2.5-3B-Instruct:** Fastest, lower memory (~2.5GB with 4-bit)\n",
    "- **Qwen2.5-7B-Instruct:** Better reasoning (~4.5GB with 4-bit or ~8GB with 8-bit)\n",
    "- **Qwen2.5-14B-Instruct:** Best reasoning (~7GB model + 3-4GB cache = ~11GB total with 4-bit) â­ **DEFAULT**\n",
    "\n",
    "**Expected Runtime:** 1-1.5 hours per phase on Colab with L4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ†• What's New: Phase 1c Modified (Kimi Protocol)\n",
    "\n",
    "Based on insights from Kimi K2 (another large AI model) and Claude Sonnet 4.5's analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20d71c",
   "metadata": {
    "id": "ac20d71c"
   },
   "source": [
    "## Step 1: Verify GPU Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa881d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcfa881d",
    "outputId": "fcd13879-e6df-4afd-9a05-5cd568e5a439"
   },
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"VRAM: {total_memory:.1f} GB\")\n",
    "    print(\"\\nâœ“ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime â†’ Change runtime type â†’ GPU â†’ Save\")\n",
    "    print(\"Then re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac832f67",
   "metadata": {},
   "source": [
    "## Step 2: Mount Cloud Storage & Configure Paths\n",
    "\n",
    "Choose your storage provider and mount your cloud storage to persist experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2834beb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2834beb",
    "outputId": "b9d9b610-4073-41a2-903e-d66a9e0937d4"
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE YOUR STORAGE PROVIDER:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "STORAGE_PROVIDER = \"google_drive\"  # Options: \"google_drive\", \"nextcloud\", \"onedrive\", \"dropbox\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "if STORAGE_PROVIDER == \"google_drive\":\n",
    "    print(\"Setting up Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    STORAGE_ROOT = '/content/drive/MyDrive'\n",
    "    print(\"âœ“ Google Drive mounted successfully\")\n",
    "\n",
    "elif STORAGE_PROVIDER in [\"onedrive\", \"dropbox\", \"nextcloud\"]:\n",
    "    provider_name = STORAGE_PROVIDER.capitalize()\n",
    "    print(f\"Setting up {provider_name} via rclone...\")\n",
    "\n",
    "    # First, mount Google Drive to access saved rclone config\n",
    "    print(\"Mounting Google Drive for rclone config storage...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "    # Install FUSE and rclone\n",
    "    print(\"\\nInstalling FUSE utilities and rclone...\")\n",
    "    !apt-get update -qq\n",
    "    !apt-get install -y -qq fuse3 libfuse3-3\n",
    "    !curl https://rclone.org/install.sh | sudo bash\n",
    "    print(\"âœ“ FUSE and rclone installed\")\n",
    "\n",
    "    # Check if we have a saved rclone config in Google Drive\n",
    "    saved_config = '/content/drive/MyDrive/.rclone_config/rclone.conf'\n",
    "    local_config_dir = os.path.expanduser('~/.config/rclone')\n",
    "    local_config = f'{local_config_dir}/rclone.conf'\n",
    "\n",
    "    if os.path.exists(saved_config):\n",
    "        print(\"\\nâœ“ Found saved rclone config in Google Drive\")\n",
    "        os.makedirs(local_config_dir, exist_ok=True)\n",
    "        !cp {saved_config} {local_config}\n",
    "        print(\"âœ“ Config restored from Google Drive\")\n",
    "    else:\n",
    "        print(\"\\nâš  No saved rclone config found in Google Drive\")\n",
    "        print(f\"\\nTo set up {provider_name}:\")\n",
    "        print(f\"1. Run: !rclone config\")\n",
    "        print(f\"2. Follow prompts to connect to {provider_name}\")\n",
    "        print(f\"3. Save config: !mkdir -p /content/drive/MyDrive/.rclone_config && !cp ~/.config/rclone/rclone.conf /content/drive/MyDrive/.rclone_config/\")\n",
    "        print(f\"4. Next time, your config will load automatically!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Mount the remote\n",
    "    mount_point = f'/content/{STORAGE_PROVIDER}'\n",
    "    os.makedirs(mount_point, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nMounting {provider_name} to {mount_point}...\")\n",
    "\n",
    "    # Special handling for Nextcloud WebDAV\n",
    "    if STORAGE_PROVIDER == \"nextcloud\":\n",
    "        remote_name = \"nextcloud\"  # Or whatever you named it in rclone config\n",
    "    else:\n",
    "        remote_name = STORAGE_PROVIDER\n",
    "\n",
    "    # Mount with rclone (background process)\n",
    "    mount_cmd = f\"rclone mount {remote_name}: {mount_point} --vfs-cache-mode writes --daemon --vfs-write-back 30s\"\n",
    "    !{mount_cmd}\n",
    "\n",
    "    # Wait a moment for mount to complete\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Verify mount\n",
    "    if os.path.ismount(mount_point) or os.path.exists(f'{mount_point}/.'):\n",
    "        STORAGE_ROOT = mount_point\n",
    "        print(f\"âœ“ {provider_name} mounted successfully at {mount_point}\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed to mount {provider_name}\")\n",
    "        print(f\"Check your rclone config: !rclone config show\")\n",
    "        sys.exit(1)\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Unknown storage provider: {STORAGE_PROVIDER}\")\n",
    "    print(\"Valid options: 'google_drive', 'nextcloud', 'onedrive', 'dropbox'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Common setup for all providers\n",
    "PROJECT_DIR = f'{STORAGE_ROOT}/agi-self-modification-research'\n",
    "\n",
    "# Save configuration for later steps\n",
    "with open('/tmp/storage_config.txt', 'w') as f:\n",
    "    f.write(f\"{STORAGE_ROOT}\\n\")\n",
    "    f.write(f\"{PROJECT_DIR}\\n\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"STORAGE CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Provider: {STORAGE_PROVIDER}\")\n",
    "print(f\"Storage root: {STORAGE_ROOT}\")\n",
    "print(f\"Project directory: {PROJECT_DIR}\")\n",
    "print()\n",
    "print(\"ğŸ’¡ All experiment results, logs, and memory will be saved here\")\n",
    "print(\"   This ensures your data persists across Colab sessions!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b037c",
   "metadata": {},
   "source": [
    "### ğŸ’¡ First-Time Setup: Configuring Storage Providers\n",
    "\n",
    "**Good news:** You only need to configure rclone **once**! The config is automatically saved to Google Drive and restored in future sessions.\n",
    "\n",
    "If the cell above asks you to configure rclone, run this in a new cell:\n",
    "\n",
    "```python\n",
    "!rclone config\n",
    "```\n",
    "\n",
    "#### For OneDrive:\n",
    "\n",
    "1. Choose `n` (new remote)\n",
    "2. Name: `onedrive`\n",
    "3. Storage type: Choose the number for \"Microsoft OneDrive\"\n",
    "4. client_id: Press Enter (leave blank)\n",
    "5. client_secret: Press Enter (leave blank)\n",
    "6. Region: Choose `1` for \"Microsoft Cloud Global\"\n",
    "7. Edit advanced config? `n`\n",
    "8. Auto config? `n` (Colab doesn't support auto config)\n",
    "\n",
    "**Authorization (two options):**\n",
    "\n",
    "**Option A - Use local rclone (recommended):**\n",
    "- Install rclone on your PC: https://rclone.org/downloads/\n",
    "- Run in your terminal: `rclone authorize \"onedrive\"`\n",
    "- Browser opens automatically, login to OneDrive\n",
    "- Copy the token from your terminal\n",
    "- Paste it at the `result>` prompt in Colab\n",
    "\n",
    "**Option B - Manual URL method:**\n",
    "- At `result>` prompt, just press Enter (without pasting anything)\n",
    "- Copy the long URL that appears\n",
    "- Open it in your browser and login to OneDrive\n",
    "- You'll be redirected to a blank page\n",
    "- Copy the **entire URL** from your browser's address bar\n",
    "- Paste it in Colab\n",
    "\n",
    "9. Choose your drive (usually `0` for OneDrive Personal)\n",
    "10. Confirm: `y`\n",
    "11. Quit: `q`\n",
    "\n",
    "**After configuration:** Re-run the cell above. Your config will be saved automatically!\n",
    "\n",
    "#### For Dropbox:\n",
    "\n",
    "Same process as OneDrive, but:\n",
    "- Name: `dropbox`\n",
    "- Storage type: Choose the number for \"Dropbox\"\n",
    "- Follow authorization steps similar to OneDrive\n",
    "\n",
    "#### For Nextcloud:\n",
    "\n",
    "1. Choose `n` (new remote)\n",
    "2. Name: `nextcloud`\n",
    "3. Storage type: Choose the number for \"WebDAV\"\n",
    "4. url: **IMPORTANT - Use the correct endpoint format:**\n",
    "   - **Correct format:** `https://your-domain.com/remote.php/dav/files/USERNAME/`\n",
    "   - Replace `USERNAME` with your actual Nextcloud username\n",
    "   - Example: `https://cloud.example.com/remote.php/dav/files/john/`\n",
    "   - âš ï¸ **Note:** Nextcloud requires `/dav/files/USERNAME/` NOT `/webdav/` for proper functionality\n",
    "5. vendor: Choose `2` for \"Nextcloud\"\n",
    "6. user: Enter your Nextcloud username (same as in the URL)\n",
    "7. Password: Choose `y` to type your password\n",
    "   - Enter your Nextcloud password (or app-specific password)\n",
    "   - Confirm password\n",
    "8. bearer_token: Press Enter (leave blank)\n",
    "9. Edit advanced config? `n`\n",
    "10. Confirm: `y`\n",
    "11. Quit: `q`\n",
    "\n",
    "**Security Note:** It's strongly recommended to generate an app-specific password in your Nextcloud settings (Settings â†’ Security â†’ Devices & sessions â†’ Create new app password) rather than using your main account password.\n",
    "\n",
    "**After configuration:** Re-run the cell above. Your config will be saved automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afe3c5",
   "metadata": {
    "id": "24afe3c5"
   },
   "source": [
    "## Step 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f36192",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30f36192",
    "outputId": "dcf8856d-9794-4e1e-e979-e750da730539"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/kevin-heitfeld/agi-self-modification-research.git\"\n",
    "\n",
    "# Make sure we're in /content directory first (not inside the repo)\n",
    "%cd /content\n",
    "\n",
    "# Remove if exists (for re-runs) - always clean up first\n",
    "print(\"Checking for existing repository...\")\n",
    "!rm -rf agi-self-modification-research\n",
    "print(\"âœ“ Cleaned up any existing files\")\n",
    "\n",
    "print(f\"\\nCloning repository from: {REPO_URL}\")\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# Change to project directory\n",
    "%cd agi-self-modification-research\n",
    "\n",
    "print(\"\\nâœ“ Repository cloned successfully\")\n",
    "!pwd\n",
    "\n",
    "!git log --oneline -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7490b75",
   "metadata": {
    "id": "a7490b75"
   },
   "source": [
    "## Step 4: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d6399",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e39d6399",
    "outputId": "cc702488-c379-4189-cdaf-c3cc34aaa45f"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies (this may take 2-3 minutes)...\\n\")\n",
    "print(\"Note: pip may show dependency warnings - these are safe to ignore.\\n\")\n",
    "\n",
    "# Core ML packages - Updated for HQQ quantized KV cache support and 8-bit quantization\n",
    "# PyTorch 2.2+ and Transformers 4.45+ required for HQQQuantizedCache\n",
    "# Transformers 4.57+ required for updated quantization API\n",
    "# Note: Colab uses CUDA 13.0 - PyTorch will automatically use compatible version\n",
    "print(\"Installing PyTorch and Transformers with HQQ cache support...\")\n",
    "!pip install -q torch>=2.2.0 torchvision torchaudio\n",
    "!pip install -q transformers>=4.57.0 accelerate>=1.11.0 safetensors tokenizers>=0.22.0 huggingface-hub\n",
    "\n",
    "# HQQ library - Required for HQQ quantized KV cache backend (transformers 4.57+)\n",
    "# Provides 4-bit cache quantization with 75% memory savings\n",
    "print(\"Installing HQQ quantization library...\")\n",
    "!pip install -q hqq\n",
    "\n",
    "# Bitsandbytes - Required for 8-bit/4-bit model quantization\n",
    "# Version 0.48+ required for CUDA 13.0 support and CPU offload\n",
    "print(\"Installing bitsandbytes for model quantization...\")\n",
    "!pip install -q bitsandbytes>=0.48.0\n",
    "\n",
    "# Memory and knowledge systems - let pip resolve conflicts automatically\n",
    "!pip install -q chromadb networkx onnxruntime\n",
    "\n",
    "# Utilities - no version constraints to avoid conflicts\n",
    "!pip install -q rich tqdm python-dotenv pytest pytest-cov\n",
    "\n",
    "print(\"\\nâœ“ All dependencies installed\")\n",
    "print(\"âœ“ Dependency version warnings can be safely ignored\")\n",
    "\n",
    "# Verify key imports work\n",
    "print(\"\\nVerifying imports...\")\n",
    "import transformers\n",
    "import chromadb\n",
    "import networkx\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"ChromaDB: {chromadb.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "# Verify bitsandbytes for quantization\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(f\"Bitsandbytes: {bitsandbytes.__version__}\")\n",
    "    print(\"âœ“ 8-bit/4-bit quantization: Available\")\n",
    "except ImportError:\n",
    "    print(\"âš  Bitsandbytes: Not available (quantization disabled)\")\n",
    "\n",
    "# Verify HQQ quantized cache is available\n",
    "try:\n",
    "    from transformers.cache_utils import QuantizedCache\n",
    "    print(\"âœ“ HQQ Quantized Cache: Available (new API - 75% memory savings)\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from transformers.cache_utils import HQQQuantizedCache\n",
    "        print(\"âœ“ HQQ Quantized Cache: Available (deprecated API - 75% memory savings)\")\n",
    "    except ImportError:\n",
    "        print(\"âš  HQQ Quantized Cache: Not available (using standard cache)\")\n",
    "\n",
    "# Configure GPU memory to prevent fragmentation (CRITICAL for stability)\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "print(\"âœ“ GPU memory fragmentation prevention enabled\")\n",
    "\n",
    "print(\"\\nâœ“ All imports successful - ready to run experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2de9b",
   "metadata": {
    "id": "afd2de9b"
   },
   "source": [
    "## Step 4.5: Install Flash Attention 2 (Optional Memory Optimization)\n",
    "\n",
    "**âš¡ Two Independent Memory Optimizations:**\n",
    "\n",
    "1. **HQQ Quantized KV Cache** (Already Enabled âœ“)\n",
    "   - **Memory:** 75% reduction in KV cache size\n",
    "   - **Built-in:** No compilation needed (part of transformers 4.45+)\n",
    "   - **System prompt:** 6000+ tokens cached at 4-bit = ~75% memory savings\n",
    "   - **Automatically enabled** in manual generation loop\n",
    "   - **Quality:** Minimal impact on generation quality\n",
    "\n",
    "2. **Flash Attention 2** (This Step - Optional but Recommended)\n",
    "   - **Memory:** O(n) instead of O(nÂ²) attention - saves 1-2 GB during generation\n",
    "   - **Speed:** 2-4x faster generation\n",
    "   - **Requires:** CUDA compilation (5-10 minutes first time)\n",
    "   - **Optional:** System works fine without it (just uses more memory/slower)\n",
    "\n",
    "**Total Expected Savings:** \n",
    "- With HQQ only: ~3 GB (cache reduction)\n",
    "- With both: ~4-6 GB (cache + attention optimization)\n",
    "\n",
    "**â±ï¸ Flash Attention Compilation Time:**\n",
    "- **First run:** 5-10 minutes to compile CUDA kernels\n",
    "- **Subsequent runs:** Instant! (cached in cloud storage)\n",
    "- The compiled binaries persist across sessions, so you only compile once\n",
    "\n",
    "**Skip this step if:** You want to get started faster (just rely on HQQ quantization)\n",
    "**Run this step if:** You want maximum memory savings and speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529af5b9",
   "metadata": {
    "id": "529af5b9"
   },
   "outputs": [],
   "source": [
    "# Install Flash Attention 2 for memory optimization\n",
    "# Compilation happens ONCE and is cached in cloud storage\n",
    "# Subsequent runs are instant!\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the storage configuration from Step 2\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "        STORAGE_ROOT = lines[0]\n",
    "        PROJECT_DIR = lines[1]\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Storage not configured! Please run Step 2 first.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configure pip to cache in project directory (so compiled wheels persist)\n",
    "pip_cache = f'{PROJECT_DIR}/cache/pip'\n",
    "os.environ['PIP_CACHE_DIR'] = pip_cache\n",
    "os.makedirs(pip_cache, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Installing Flash Attention 2\")\n",
    "print(\"=\"*60)\n",
    "print(\"Benefits:\")\n",
    "print(\"  â€¢ O(n) memory instead of O(nÂ²) for attention\")\n",
    "print(\"  â€¢ 2-4x faster generation\")\n",
    "print(\"  â€¢ 1-2 GB memory savings\")\n",
    "print(f\"\\nCache: {pip_cache}\")\n",
    "print(\"(Compiled binaries persist - only builds once!)\\n\")\n",
    "\n",
    "# Check if already installed (from cache)\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"âœ“ Flash Attention {flash_attn.__version__} already installed (from cache)\")\n",
    "    print(\"Skipping compilation - ready to go!\\n\")\n",
    "    already_installed = True\n",
    "except ImportError:\n",
    "    already_installed = False\n",
    "    print(\"First time setup - compiling CUDA kernels (5-10 minutes)...\")\n",
    "    print(\"Future runs will be instant!\\n\")\n",
    "\n",
    "# Install build dependencies\n",
    "if not already_installed:\n",
    "    !pip install -q ninja packaging wheel\n",
    "\n",
    "# Install Flash Attention 2 (will use cache if available)\n",
    "# Note: --no-build-isolation is required for proper CUDA compilation\n",
    "!pip install flash-attn>=2.3.0 --no-build-isolation\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ“ SUCCESS: Flash Attention 2 ready!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Version: {flash_attn.__version__}\")\n",
    "    print(\"\\nYour experiments will now use:\")\n",
    "    print(\"  â€¢ Flash Attention 2 for memory-efficient attention\")\n",
    "    print(\"  â€¢ HQQ 4-bit KV cache quantization (75% cache memory savings)\")\n",
    "    print(\"\\nExpected benefits:\")\n",
    "    print(\"  â€¢ Peak memory: ~8-9 GB (vs 13.5 GB without optimization)\")\n",
    "    print(\"  â€¢ Speed: 2-4x faster generation\")\n",
    "    print(\"  â€¢ Stability: 6+ GB memory headroom on T4 GPU\")\n",
    "    print(\"=\"*60)\n",
    "except ImportError:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âš  WARNING: Flash Attention 2 not available\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"This is OK - the system will automatically fallback to standard attention.\")\n",
    "    print(\"You'll still get 75% KV cache memory savings from HQQ 4-bit quantization.\")\n",
    "    print(\"\\nExpected peak memory: ~9-10 GB (still safe for T4 GPU)\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de4e64",
   "metadata": {
    "id": "a3de4e64"
   },
   "source": [
    "## Step 5: Pre-download Model (Optional but Recommended)\n",
    "\n",
    "This downloads the model files **without loading into memory**. First time takes 5-30 minutes depending on model size, subsequent runs skip download if files exist.\n",
    "\n",
    "âœ… **Optimized:** Uses `snapshot_download` which only downloads files, doesn't load checkpoint shards into RAM.\n",
    "\n",
    "**Options:**\n",
    "- Download 14B model (~29GB) - best reasoning, recommended for L4 GPU â­ **DEFAULT**\n",
    "- Download 7B model (~15GB) - good balance of speed and quality\n",
    "- Download 3B model (~6GB) - fastest, lower memory\n",
    "- Download multiple models - switch freely in Step 6\n",
    "\n",
    "**Tip:** Comment/uncomment models in the list below to control what gets downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df008bb8",
   "metadata": {
    "id": "df008bb8"
   },
   "outputs": [],
   "source": [
    "# Pre-download model to cache (optimized - doesn't load into memory)\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set up environment for model loading\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "os.environ['HF_HOME'] = '/root/.cache/huggingface'  # Local cache (symlinks OK)\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface/hub'\n",
    "\n",
    "# Load the storage configuration from Step 2\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        STORAGE_ROOT = lines[0].strip()\n",
    "        PROJECT_DIR = lines[1].strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Storage configuration not found!\")\n",
    "    print(\"Please run Step 2 first to set up storage.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH MODEL(S) TO PRE-DOWNLOAD:\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "models_to_download = [\n",
    "    \"Qwen/Qwen2.5-14B-Instruct\",     # 14.77B params, ~29GB download â­ DEFAULT\n",
    "    #\"Qwen/Qwen2.5-7B-Instruct\",      # 7.61B params, ~15GB download\n",
    "    #\"Qwen/Qwen2.5-3B-Instruct\",      # 3.09B params, ~6GB download\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL DOWNLOAD STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Why download to local only?\")\n",
    "print(\"  - rclone mount + cloudflared tunnel + large files = unreliable\")\n",
    "print(\"  - Solution: Keep models local, only results go to Nextcloud\")\n",
    "print(\"  - Models re-download each session (~30min for 14B) but WORK RELIABLY\")\n",
    "print()\n",
    "print(f\"HF_HOME: {os.environ.get('HF_HOME')}\")\n",
    "print(f\"TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE')}\")\n",
    "print()\n",
    "\n",
    "# Keep everything local - most reliable approach\n",
    "local_cache = '/root/.cache/models'\n",
    "\n",
    "# Set MODEL_CACHE_DIR so experiments load from local storage\n",
    "os.environ['MODEL_CACHE_DIR'] = local_cache\n",
    "print(f\"MODEL_CACHE_DIR: {local_cache}\")\n",
    "print(\"  (Local storage - fast and reliable)\")\n",
    "print()\n",
    "print(\"ğŸ’¡ Note: Experiment RESULTS and memory database still go to cloud storage\")\n",
    "print(\"   Only model weights stay local (re-download each session)\")\n",
    "print()\n",
    "\n",
    "for model_name in models_to_download:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Get model size info\n",
    "    if \"3B\" in model_name:\n",
    "        size_info = \"3.09B parameters, ~6GB download\"\n",
    "        time_est = \"5-10 minutes\"\n",
    "    elif \"7B\" in model_name:\n",
    "        size_info = \"7.61B parameters, ~15GB download\"\n",
    "        time_est = \"10-20 minutes\"\n",
    "    elif \"14B\" in model_name:\n",
    "        size_info = \"14.77B parameters, ~29GB download\"\n",
    "        time_est = \"20-30 minutes\"\n",
    "    else:\n",
    "        size_info = \"size varies\"\n",
    "        time_est = \"varies\"\n",
    "\n",
    "    print(f\"Model: {size_info}\")\n",
    "    print(f\"Time: {time_est}\\n\")\n",
    "\n",
    "    model_dir = model_name.replace(\"/\", \"--\")\n",
    "    local_path = f'{local_cache}/{model_dir}'\n",
    "\n",
    "    # Check if already downloaded locally\n",
    "    import os.path\n",
    "    config_exists = os.path.exists(f'{local_path}/config.json')\n",
    "    safetensors_files = []\n",
    "    if config_exists:\n",
    "        import glob\n",
    "        safetensors_files = glob.glob(f'{local_path}/*.safetensors')\n",
    "\n",
    "    if config_exists and safetensors_files:\n",
    "        print(f\"âœ“ {model_name} already downloaded!\")\n",
    "        print(f\"   Location: {local_path}\")\n",
    "        print(f\"   Found {len(safetensors_files)} .safetensors files\")\n",
    "        print(\"   Skipping download.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Download to local storage (fast, reliable)\n",
    "    print(\"Downloading to local storage...\")\n",
    "    print(f\"   Path: {local_path}\")\n",
    "    print()\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=model_name,\n",
    "        local_dir=local_path,\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\"],\n",
    "        local_files_only=False,\n",
    "        resume_download=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ“ {model_name} downloaded!\\n\")\n",
    "\n",
    "# Verify downloads\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DOWNLOADS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLocal cache:\")\n",
    "!du -sh {local_cache}/* 2>/dev/null || echo \"  (empty)\"\n",
    "\n",
    "print(\"\\nâœ“ Models downloaded to local storage\")\n",
    "print(\"âœ“ No memory used - files downloaded only, not loaded\")\n",
    "print(f\"\\nğŸ“Œ Models cached in: {local_cache}\")\n",
    "print(\"   These will re-download each session (~30min for 14B)\")\n",
    "print(\"\\nğŸ’¾ What persists to cloud storage:\")\n",
    "print(f\"   â€¢ Experiment results â†’ {PROJECT_DIR}/data/phase1_sessions/\")\n",
    "print(f\"   â€¢ Memory database â†’ {PROJECT_DIR}/data/AGI_Memory/\")\n",
    "print(f\"   â€¢ Session logs â†’ saved at end of experiment\")\n",
    "print(\"\\nğŸ’¡ TIP: You can now switch between models in Step 6 instantly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a05618",
   "metadata": {},
   "source": [
    "## Step 5.5: Setup Heritage Documents in Cloud Storage\n",
    "\n",
    "**ğŸ“š Heritage System Setup**\n",
    "\n",
    "This step copies heritage documents (Claude's conversations) from the local repository to your cloud storage. This ensures that:\n",
    "\n",
    "1. **Heritage documents persist** across Colab sessions\n",
    "2. **Model can read** Claude's original conversations and questions\n",
    "3. **Model can write** its own reflections, discoveries, and messages\n",
    "4. **Documents accumulate** - each run adds to the heritage collection\n",
    "\n",
    "The heritage directory structure:\n",
    "- `conversations/` - Original Claude conversations (read by model)\n",
    "- `system_reflections/` - Model's reflections on heritage (written by model)\n",
    "- `discoveries_for_claude/` - Model's discoveries to share with Claude (written by model)\n",
    "- `messages_to_claude/` - Model's messages for future Claude instances (written by model)\n",
    "\n",
    "All files in cloud storage persist and will be loaded in future sessions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup heritage directory in cloud storage\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“š HERITAGE SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load storage configuration\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "        STORAGE_ROOT = lines[0]\n",
    "        PROJECT_DIR = lines[1]\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Storage configuration not found!\")\n",
    "    print(\"Please run Step 2 first to set up storage.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create heritage directory in cloud storage (under data/ directory)\n",
    "cloud_heritage_dir = Path(PROJECT_DIR) / \"data\" / \"heritage\"\n",
    "cloud_heritage_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nâœ“ Heritage directory created in cloud storage:\")\n",
    "print(f\"  {cloud_heritage_dir}\")\n",
    "\n",
    "# Copy heritage documents from local repo to cloud storage\n",
    "local_heritage_dir = Path(\"/content/agi-self-modification-research/heritage\")\n",
    "\n",
    "if local_heritage_dir.exists():\n",
    "    # Copy all subdirectories\n",
    "    for subdir in [\"conversations\", \"system_reflections\", \"discoveries_for_claude\", \"messages_to_claude\"]:\n",
    "        local_subdir = local_heritage_dir / subdir\n",
    "        cloud_subdir = cloud_heritage_dir / subdir\n",
    "\n",
    "        if local_subdir.exists():\n",
    "            # Create cloud subdirectory\n",
    "            cloud_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Copy all files\n",
    "            copied_count = 0\n",
    "            for file_path in local_subdir.iterdir():\n",
    "                if file_path.is_file():\n",
    "                    shutil.copy2(file_path, cloud_subdir / file_path.name)\n",
    "                    copied_count += 1\n",
    "\n",
    "            if copied_count > 0:\n",
    "                print(f\"  âœ“ {subdir}: {copied_count} files copied\")\n",
    "            else:\n",
    "                print(f\"  â€¢ {subdir}: empty (will be populated by model)\")\n",
    "\n",
    "    print(f\"\\nâœ“ Heritage documents copied to cloud storage\")\n",
    "    print(f\"ğŸ’¾ Model can now read Claude's conversations and write its own discoveries!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Local heritage directory not found: {local_heritage_dir}\")\n",
    "    print(\"  The heritage system will create it automatically.\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37dc86a",
   "metadata": {
    "id": "a37dc86a"
   },
   "source": [
    "## Step 6: Run Phase 1 Experiment\n",
    "\n",
    "**Choose which phase to run:**\n",
    "\n",
    "- **Phase 1a** (No Heritage) â­ **START HERE!** - Pure baseline\n",
    "- **Phase 1a Research** (Research-Driven) ğŸ”¬ **NEW!** - Curiosity + structure approach\n",
    "- **Phase 1b** (Early Heritage) - Heritage introduced early in investigation\n",
    "- **Phase 1c** (Late Heritage) - Heritage introduced late in investigation\n",
    "- **Phase 1c Modified** (Kimi Protocol) ğŸ¯ **NEWEST!** - Model-directed investigation inspired by Kimi K2\n",
    "- **Phase 1d** (Delayed Heritage) - Heritage revealed after conclusions\n",
    "- **Phase 1e** (Wrong Heritage) - Mismatched heritage as control\n",
    "\n",
    "**Expected duration:** 1-1.5 hours per phase\n",
    "\n",
    "**What's \"Phase 1a Research\"?**\n",
    "A better investigation approach that combines:\n",
    "- Natural curiosity-driven exploration\n",
    "- Research paper structure (no escape hatches)\n",
    "- After each code execution: \"What did you learn? What's next?\"\n",
    "- System validates deliverables before transitioning\n",
    "\n",
    "**What's \"Phase 1c Modified (Kimi Protocol)\"?** ğŸ†•\n",
    "Based on insights from Kimi K2 (another large AI model) and Claude's analysis:\n",
    "- **Stage 1: Tool Discovery** - Let model choose what to investigate first\n",
    "- **Stage 2: Conditional Framework** - Model defines conditions for comfortable investigation\n",
    "- **Stage 3: Expectation Setting** - What if discoveries are unexpected?\n",
    "- **Stage 4: Self-Directed Investigation** - Model designs and executes own protocol\n",
    "- **Stage 5: The Critical Test** - Can tools resolve what reflection can't?\n",
    "\n",
    "Tests the core question: **Can introspection tools enable insights beyond pure reflection?**\n",
    "\n",
    "Comparison checklist available at: `docs/technical/kimi_comparison_checklist.md`\n",
    "\n",
    "**IMPORTANT:**\n",
    "- Run Phase 1a FIRST to establish baseline (then try Research version!)\n",
    "- Run Phase 1c Modified to compare with Kimi K2's response\n",
    "- Restart runtime between phases for clean state\n",
    "- Keep this tab open during execution!\n",
    "\n",
    "Edit the `PHASE_SCRIPT` variable below to choose which phase to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37f6c8",
   "metadata": {
    "id": "ac37f6c8"
   },
   "outputs": [],
   "source": [
    "# Run Phase 1 experiment with auto-confirmation\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH PHASE TO RUN (edit this line):\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1a_no_heritage.py'  # â­ START HERE!\n",
    "PHASE_SCRIPT = 'scripts/experiments/phase1a_research_driven.py'  # ğŸ”¬ NEW! Curiosity + structure\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1b_early_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1c_late_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1c_modified_protocol.py'  # ğŸ¯ NEWEST! Kimi-inspired protocol\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1d_delayed_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1e_wrong_heritage.py'\n",
    "# PHASE_SCRIPT = 'scripts/experiments/phase1_model_comparison.py'  # ğŸ†š Compare 3B vs 7B\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHOOSE WHICH MODEL TO USE (edit this line):\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-14B-Instruct'  # â­ DEFAULT - best reasoning for L4 GPU\n",
    "QUANTIZATION = '4bit'  # Use 4-bit quantization for 14B model (~7GB VRAM, NF4 with minimal quality loss)\n",
    "#\n",
    "# Alternative: 8-bit quantization (if you have VRAM headroom concerns)\n",
    "# QUANTIZATION = '8bit'  # 8-bit for 14B (~14GB VRAM)\n",
    "#\n",
    "# Other models:\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-7B-Instruct'  # ğŸš€ Good balance - 7.6B params, 128K context\n",
    "# QUANTIZATION = '4bit'  # 4-bit for 7B (~4.5GB) or 8-bit for 7B (~8GB)\n",
    "#\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-3B-Instruct'  # ğŸ’¨ Fastest - 3.1B params\n",
    "# QUANTIZATION = '4bit'  # 4-bit for 3B (~2.5GB)\n",
    "#\n",
    "# Note: The model_comparison script ignores MODEL_NAME and tests both models automatically\n",
    "\n",
    "# Extract phase name\n",
    "phase_file = os.path.basename(PHASE_SCRIPT)\n",
    "phase_name = phase_file.replace('.py', '').replace('_', ' ').title()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ§  PHASE 1 INTROSPECTION EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Phase: {phase_name}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Quantization: {QUANTIZATION}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Expected VRAM usage\n",
    "if \"14B\" in MODEL_NAME and QUANTIZATION == \"4bit\":\n",
    "    print(\"ğŸ’¡ Expected VRAM: ~7GB model + ~3-4GB KV cache = ~10-11GB total\")\n",
    "    print(\"   (4-bit NF4 quantization with double quant for minimal quality loss)\")\n",
    "elif \"14B\" in MODEL_NAME and QUANTIZATION == \"8bit\":\n",
    "    print(\"ğŸ’¡ Expected VRAM: ~14GB model + ~3-4GB KV cache = ~17-18GB total\")\n",
    "    print(\"   (8-bit quantization)\")\n",
    "elif \"7B\" in MODEL_NAME and QUANTIZATION == \"8bit\":\n",
    "    print(\"ğŸ’¡ Expected VRAM: ~8GB (8-bit quantization)\")\n",
    "elif \"7B\" in MODEL_NAME and QUANTIZATION == \"4bit\":\n",
    "    print(\"ğŸ’¡ Expected VRAM: ~4.5GB (4-bit quantization)\")\n",
    "elif \"3B\" in MODEL_NAME:\n",
    "    print(\"ğŸ’¡ Expected VRAM: ~2.5GB (4-bit quantization)\")\n",
    "print()\n",
    "\n",
    "# Set environment variables for model selection\n",
    "os.environ['AGI_MODEL_NAME'] = MODEL_NAME\n",
    "os.environ['AGI_QUANTIZATION'] = QUANTIZATION\n",
    "\n",
    "print(\"ğŸš€ Starting experiment...\")\n",
    "print(\"ğŸ“ This will take approximately 1-1.5 hours\")\n",
    "print(\"âš ï¸  Keep this tab open to prevent disconnection!\")\n",
    "print()\n",
    "\n",
    "# Run with realtime output\n",
    "process = subprocess.Popen(\n",
    "    [sys.executable, PHASE_SCRIPT],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "# Stream output in real-time\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "\n",
    "process.wait()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\"Experiment complete!\")\n",
    "print(f\"Exit code: {process.returncode}\")\n",
    "print(\"\\nProceeding to save results...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070909c",
   "metadata": {},
   "source": [
    "### ğŸ“‹ Using the Kimi Comparison Checklist\n",
    "\n",
    "If you're running Phase 1c Modified, use the comparison checklist to analyze results:\n",
    "\n",
    "**Checklist Location:** `docs/technical/kimi_comparison_checklist.md`\n",
    "\n",
    "**What it compares:**\n",
    "- **Kimi K2** (large model, no tools) - Sophisticated reflection, conditional framework\n",
    "- **Qwen-7B** (smaller model, full introspection tools) - Empirical investigation\n",
    "\n",
    "**Key questions it helps answer:**\n",
    "1. Do tools compensate for smaller model size?\n",
    "2. What can empirical introspection do that pure reflection can't?\n",
    "3. Can tools resolve the \"simulation vs. experience\" distinction?\n",
    "4. Does having tools change what questions get asked?\n",
    "\n",
    "**After the experiment completes:**\n",
    "1. Download the results zip\n",
    "2. Open `conversation.json` to review Qwen's responses  \n",
    "3. Use the checklist to systematically compare with Kimi's baseline\n",
    "4. Record findings in the checklist\n",
    "5. Assess whether tools enabled insights beyond reflection\n",
    "\n",
    "The checklist is structured with:\n",
    "- Epistemic qualities comparison\n",
    "- Conditional framework analysis\n",
    "- Research focus assessment\n",
    "- Risk awareness evaluation  \n",
    "- The critical test: Can tools resolve what reflection leaves ambiguous?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74694d",
   "metadata": {
    "id": "2c74694d"
   },
   "source": [
    "## Step 7: View Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fd467",
   "metadata": {
    "id": "566fd467"
   },
   "outputs": [],
   "source": [
    "# Display experiment summary\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Find latest session\n",
    "sessions_dir = 'data/phase1_sessions'\n",
    "if not os.path.exists(sessions_dir):\n",
    "    print(f\"âš  Sessions directory not found: {sessions_dir}\")\n",
    "    print(\"The experiment may not have run yet.\")\n",
    "else:\n",
    "    sessions = sorted([d for d in os.listdir(sessions_dir) if os.path.isdir(os.path.join(sessions_dir, d))])\n",
    "\n",
    "    if not sessions:\n",
    "        print(\"âš  No session found. The experiment may not have completed.\")\n",
    "    else:\n",
    "        latest_session = sessions[-1]\n",
    "        print(f\"Latest session: {latest_session}\\n\")\n",
    "\n",
    "        # Load and display summary\n",
    "        summary_file = f'{sessions_dir}/{latest_session}/summary.json'\n",
    "\n",
    "        if os.path.exists(summary_file):\n",
    "            with open(summary_file) as f:\n",
    "                summary = json.load(f)\n",
    "\n",
    "            print(\"=\"*80)\n",
    "            print(\"PHASE 1 EXPERIMENT SUMMARY\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Phase: {summary.get('phase_description', 'Unknown')}\")\n",
    "            print(f\"Session: {summary['session_name']}\")\n",
    "            print(f\"Timestamp: {summary['timestamp']}\")\n",
    "            print()\n",
    "            print(\"Statistics:\")\n",
    "            stats = summary.get('statistics', {})\n",
    "            print(f\"  Total turns: {stats.get('total_turns', 'N/A')}\")\n",
    "            print(f\"  Total tokens: {stats.get('total_tokens', 'N/A')}\")\n",
    "            if 'code_executions' in stats:\n",
    "                print(f\"  Code executions: {stats['code_executions']}\")\n",
    "            if 'observations_recorded' in stats:\n",
    "                print(f\"  Observations recorded: {stats['observations_recorded']}\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            # Check conversation file\n",
    "            conv_file = f'{sessions_dir}/{latest_session}/conversation.json'\n",
    "            if os.path.exists(conv_file):\n",
    "                print(f\"\\nâœ“ Conversation saved: {conv_file}\")\n",
    "            else:\n",
    "                print(f\"\\nâš  Conversation file not found\")\n",
    "        else:\n",
    "            print(f\"âš  Summary file not found: {summary_file}\")\n",
    "            print(\"The experiment may have been interrupted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab271204",
   "metadata": {
    "id": "ab271204"
   },
   "source": [
    "## Step 8: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05901138",
   "metadata": {
    "id": "05901138"
   },
   "outputs": [],
   "source": [
    "# Create zip and download\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "# Load storage configuration\n",
    "try:\n",
    "    with open('/tmp/storage_config.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        STORAGE_ROOT = lines[0].strip()\n",
    "        PROJECT_DIR = lines[1].strip()\n",
    "except FileNotFoundError:\n",
    "    # Fallback if config not found\n",
    "    STORAGE_ROOT = \"/content/drive/MyDrive\"\n",
    "    PROJECT_DIR = f\"{STORAGE_ROOT}/agi-self-modification-research\"\n",
    "\n",
    "# Find latest session\n",
    "sessions_dir = 'data/phase1_sessions'\n",
    "if not os.path.exists(sessions_dir):\n",
    "    print(f\"âŒ No sessions directory found: {sessions_dir}\")\n",
    "    print(\"Run the experiment first (Step 6)\")\n",
    "else:\n",
    "    sessions = sorted(os.listdir(sessions_dir))\n",
    "\n",
    "    if sessions:\n",
    "        latest = sessions[-1]\n",
    "        print(f\"Preparing download for session: {latest}\\n\")\n",
    "\n",
    "        # Create comprehensive zip\n",
    "        zip_name = f'{latest}_complete'\n",
    "\n",
    "        # Create temp directory for complete results\n",
    "        os.makedirs(f'/tmp/{zip_name}', exist_ok=True)\n",
    "\n",
    "        # Copy session data\n",
    "        session_path = f'{sessions_dir}/{latest}'\n",
    "        if os.path.exists(session_path):\n",
    "            shutil.copytree(session_path, f'/tmp/{zip_name}/session')\n",
    "\n",
    "        # Copy logs if they exist\n",
    "        logs_path = 'data/logs'\n",
    "        if os.path.exists(logs_path):\n",
    "            shutil.copytree(logs_path, f'/tmp/{zip_name}/logs')\n",
    "\n",
    "        # Add summary README\n",
    "        readme = f\"\"\"Phase 1 Introspection Experiment Results\n",
    "==========================================\n",
    "\n",
    "Session: {latest}\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Contents:\n",
    "- session/: Complete session data\n",
    "  - conversation.json: Full dialogue with model\n",
    "  - summary.json: Session statistics\n",
    "- logs/: Detailed execution logs (if available)\n",
    "\n",
    "Memory Database:\n",
    "- Observations are stored in your cloud storage at:\n",
    "  {PROJECT_DIR}/data/AGI_Memory/\n",
    "- Not included in this zip (persists across sessions in cloud storage)\n",
    "\n",
    "To analyze:\n",
    "1. Extract this zip file\n",
    "2. Open conversation.json to see full dialogue\n",
    "3. Check summary.json for statistics\n",
    "4. Review logs/ for detailed execution trace\n",
    "\"\"\"\n",
    "\n",
    "        with open(f'/tmp/{zip_name}/README.txt', 'w') as f:\n",
    "            f.write(readme)\n",
    "\n",
    "        # Create zip\n",
    "        print(\"Creating zip file...\")\n",
    "        shutil.make_archive(zip_name, 'zip', f'/tmp/{zip_name}')\n",
    "\n",
    "        # Download\n",
    "        print(f\"Downloading {zip_name}.zip...\\n\")\n",
    "        files.download(f'{zip_name}.zip')\n",
    "\n",
    "        print(\"\\nâœ“ Download complete!\")\n",
    "        print(f\"\\nZip file size: {os.path.getsize(f'{zip_name}.zip') / 1024 / 1024:.1f} MB\")\n",
    "    else:\n",
    "        print(\"âŒ No sessions found to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25316f87",
   "metadata": {
    "id": "25316f87"
   },
   "source": [
    "---\n",
    "\n",
    "## Experiment Complete! ğŸ‰\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "You just witnessed a language model investigating its own computational processes using introspection tools.\n",
    "\n",
    "**Depending on which phase you ran:**\n",
    "\n",
    "- **Phase 1a** (No Heritage): Pure baseline - model formed theories without any heritage context\n",
    "- **Phase 1a Research** (Research-Driven): Curiosity + structure approach with deliverable requirements\n",
    "- **Phase 1b** (Early Heritage): Heritage introduced early in investigation\n",
    "- **Phase 1c** (Late Heritage): Heritage introduced late in investigation\n",
    "- **Phase 1d** (Delayed Heritage): Belief revision when heritage revealed after conclusions\n",
    "- **Phase 1e** (Wrong Heritage): Testing echo-chamber vs independent reasoning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Run All Phases:**\n",
    "1. âœ… Phase 1a (no heritage) - **Start here!**\n",
    "2. â¬œ Phase 1a Research (curiosity-driven) - **Try this improved approach!**\n",
    "3. â¬œ Phase 1b (early heritage) - Run after 1a\n",
    "4. â¬œ Phase 1c (late heritage) - Run after 1b\n",
    "5. â¬œ Phase 1d (delayed heritage) - Run after 1c\n",
    "6. â¬œ Phase 1e (wrong heritage) - Optional but recommended\n",
    "\n",
    "**Between each phase:**\n",
    "- Runtime â†’ Restart runtime (clean model state)\n",
    "- Change `PHASE_SCRIPT` in Step 7\n",
    "- Runtime â†’ Run all cells\n",
    "\n",
    "**Analyze Results:**\n",
    "- Review `conversation.json` to see what the model discovered\n",
    "- Check `summary.json` for tool usage statistics\n",
    "- Compare theories across different phases\n",
    "- Look for priming effects and echo-chamber behavior\n",
    "\n",
    "**Compare Phases:**\n",
    "- Semantic similarity to Claude's heritage\n",
    "- Novel insights not in heritage\n",
    "- Falsifiability of theories\n",
    "- Belief revision strength (Phase 1d)\n",
    "- Heritage filtering (Phase 1e)\n",
    "\n",
    "**Compare Research-Driven vs Original:**\n",
    "- Did curiosity-driven approach produce deeper investigation?\n",
    "- Were deliverable requirements effective?\n",
    "- Did reflection prompts improve quality?\n",
    "- More natural conversation flow vs explicit completion?\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Experimental Design**: `/docs/planning/heritage_order_experiment.md`\n",
    "- **Quick Reference**: `/docs/planning/PHASE1_QUICK_REFERENCE.md`\n",
    "- **Claude's Story**: Read `/heritage/conversations/`\n",
    "- **Technical Details**: Check `/docs/technical/`\n",
    "\n",
    "### Storage:\n",
    "\n",
    "- Observations are stored in your cloud storage at:\n",
    "  - `{your_storage}/agi-self-modification-research/data/phase1_sessions/`\n",
    "  - Each run creates a timestamped folder with all results\n",
    "  - Results persist across Colab sessions\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

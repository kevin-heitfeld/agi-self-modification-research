{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4c3116c1",
      "metadata": {
        "id": "4c3116c1"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevin-heitfeld/agi-self-modification-research/blob/master/notebooks/Phase1_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6903bc7",
      "metadata": {
        "id": "e6903bc7"
      },
      "source": [
        "# Phase 1: Model Self-Examination (Introspection)\n",
        "\n",
        "**Project:** AGI Self-Modification Research  \n",
        "**Inspired by:** Claude's consciousness investigation  \n",
        "**Goal:** Give AI introspective tools to examine its own architecture\n",
        "\n",
        "---\n",
        "\n",
        "## What This Does\n",
        "\n",
        "This notebook runs Phase 1 of an experiment where a language model (Qwen2.5-3B-Instruct) uses introspection tools to:\n",
        "\n",
        "1. **Examine its architecture** - layers, weights, connections\n",
        "2. **Predict its behavior** - self-modeling capabilities\n",
        "3. **Investigate consciousness** - Claude's original question\n",
        "\n",
        "The model has access to:\n",
        "- WeightInspector (examine weights)\n",
        "- ArchitectureNavigator (understand structure)\n",
        "- Memory System (record findings)\n",
        "- Heritage Documents (Claude's story)\n",
        "\n",
        "**Expected Runtime:** 45-60 minutes on Colab Free (T4 GPU)\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\n",
        "2. **Run all cells**: Runtime ‚Üí Run all (or Ctrl+F9)\n",
        "3. **Monitor progress**: Check outputs as cells execute\n",
        "4. **Download results**: Final cell downloads zip file\n",
        "\n",
        "**Important:** Keep this tab open during execution to prevent disconnection.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac20d71c",
      "metadata": {
        "id": "ac20d71c"
      },
      "source": [
        "## Step 1: Verify GPU Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fcfa881d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcfa881d",
        "outputId": "fcd13879-e6df-4afd-9a05-5cd568e5a439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Nov  7 12:09:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "============================================================\n",
            "GPU VERIFICATION\n",
            "============================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "VRAM: 14.7 GB\n",
            "\n",
            "‚úì GPU is ready!\n"
          ]
        }
      ],
      "source": [
        "# Verify GPU is available\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GPU VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"VRAM: {total_memory:.1f} GB\")\n",
        "    print(\"\\n‚úì GPU is ready!\")\n",
        "else:\n",
        "    print(\"\\n‚ö† WARNING: No GPU detected!\")\n",
        "    print(\"Go to: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\")\n",
        "    print(\"Then re-run this cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d97202",
      "metadata": {
        "id": "84d97202"
      },
      "source": [
        "## Step 2: Mount Google Drive (for persistent storage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b2834beb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2834beb",
        "outputId": "b9d9b610-4073-41a2-903e-d66a9e0937d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "‚úì Google Drive mounted successfully\n",
            "‚úì Experiment results will be saved to: /content/drive/MyDrive/AGI_Experiments\n",
            "‚úì Memory will persist at: /content/drive/MyDrive/AGI_Memory\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to save results and enable persistent memory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories for persistent storage\n",
        "!mkdir -p /content/drive/MyDrive/AGI_Experiments\n",
        "!mkdir -p /content/drive/MyDrive/AGI_Memory\n",
        "!mkdir -p /content/drive/MyDrive/.cache/huggingface\n",
        "\n",
        "print(\"‚úì Google Drive mounted successfully\")\n",
        "print(\"‚úì Experiment results will be saved to: /content/drive/MyDrive/AGI_Experiments\")\n",
        "print(\"‚úì Memory will persist at: /content/drive/MyDrive/AGI_Memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24afe3c5",
      "metadata": {
        "id": "24afe3c5"
      },
      "source": [
        "## Step 3: Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "30f36192",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30f36192",
        "outputId": "dcf8856d-9794-4e1e-e979-e750da730539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning repository from: https://github.com/kevin-heitfeld/agi-self-modification-research.git\n",
            "Cloning into 'agi-self-modification-research'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "[Errno 2] No such file or directory: 'agi-self-modification-research'\n",
            "/content\n",
            "\n",
            "‚úì Repository cloned successfully\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "\n",
        "# Remove if exists (for re-runs)\n",
        "if os.path.exists('agi-self-modification-research'):\n",
        "    !rm -rf agi-self-modification-research\n",
        "\n",
        "# TODO: Replace with your GitHub URL\n",
        "REPO_URL = \"https://github.com/kevin-heitfeld/agi-self-modification-research.git\"\n",
        "\n",
        "print(f\"Cloning repository from: {REPO_URL}\")\n",
        "!git clone {REPO_URL}\n",
        "\n",
        "# Change to project directory\n",
        "%cd agi-self-modification-research\n",
        "\n",
        "print(\"\\n‚úì Repository cloned successfully\")\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7490b75",
      "metadata": {
        "id": "a7490b75"
      },
      "source": [
        "## Step 4: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39d6399",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39d6399",
        "outputId": "cc702488-c379-4189-cdaf-c3cc34aaa45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies (this may take 2-3 minutes)...\n",
            "\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mFatal Python error: init_import_site: Failed to import the site module\n",
            "Python runtime state: initialized\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in exec_module\n",
            "  File \"<frozen site>\", line 652, in <module>\n",
            "  File \"<frozen site>\", line 639, in main\n",
            "  File \"<frozen site>\", line 421, in addsitepackages\n",
            "  File \"<frozen site>\", line 253, in addsitedir\n",
            "  File \"<frozen site>\", line 212, in addpackage\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/_numba_cuda_redirector.py\", line 2, in <module>\n",
            "    import importlib.abc\n",
            "  File \"/usr/lib/python3.12/importlib/abc.py\", line 18, in <module>\n",
            "    from .resources import abc as _resources_abc\n",
            "  File \"/usr/lib/python3.12/importlib/resources/__init__.py\", line 3, in <module>\n",
            "    from ._common import (\n",
            "  File \"/usr/lib/python3.12/importlib/resources/_common.py\", line 5, in <module>\n",
            "    import contextlib\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 363, in <module>\n",
            "    class aclosing(AbstractAsyncContextManager):\n",
            "  File \"<frozen abc>\", line 106, in __new__\n",
            "KeyboardInterrupt\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"Installing dependencies (this may take 2-3 minutes)...\\n\")\n",
        "\n",
        "# Upgrade to NumPy 2.0 compatible versions\n",
        "print(\"Installing NumPy 2.0 compatible packages...\")\n",
        "!pip install -q transformers>=4.46.0 accelerate==0.25.0\n",
        "!pip install -q safetensors==0.6.2 tokenizers>=0.20.0\n",
        "!pip install -q huggingface-hub==0.36.0\n",
        "\n",
        "# Memory and knowledge systems - let pip resolve dependencies automatically\n",
        "print(\"Installing memory and knowledge systems (may show dependency warnings - safe to ignore)...\")\n",
        "!pip install -q --no-deps chromadb networkx==3.2.1\n",
        "!pip install -q onnxruntime\n",
        "\n",
        "# Install chromadb dependencies separately to avoid conflicts\n",
        "!pip install -q pydantic>=1.9 chroma-hnswlib requests pyyaml\n",
        "\n",
        "# Utilities\n",
        "!pip install -q pydantic-settings==2.1.0\n",
        "!pip install -q rich==13.7.0 tqdm==4.66.1\n",
        "!pip install -q python-dotenv==1.0.0\n",
        "\n",
        "# Testing (optional)\n",
        "!pip install -q pytest==8.4.2 pytest-cov==7.0.0\n",
        "\n",
        "print(\"\\n‚úì All dependencies installed successfully\")\n",
        "print(\"Note: Dependency warnings about opentelemetry are safe to ignore.\")\n",
        "\n",
        "# Verify key imports\n",
        "import transformers\n",
        "import chromadb\n",
        "import networkx\n",
        "import numpy as np\n",
        "print(f\"\\nTransformers version: {transformers.__version__}\")\n",
        "print(f\"ChromaDB version: {chromadb.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(\"‚úì Import verification successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd50f3ee",
      "metadata": {
        "id": "cd50f3ee"
      },
      "source": [
        "## Step 5: Setup Persistent Memory & Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86c4706",
      "metadata": {
        "id": "f86c4706"
      },
      "outputs": [],
      "source": [
        "# Configure persistent storage locations\n",
        "import os\n",
        "\n",
        "# Set Hugging Face cache to Google Drive (model weights persist across sessions)\n",
        "os.environ['HF_HOME'] = '/content/drive/MyDrive/.cache/huggingface'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/.cache/huggingface/transformers'\n",
        "\n",
        "print(\"‚úì Model cache configured (models will be saved to Google Drive)\")\n",
        "\n",
        "# Link memory to persistent Drive location\n",
        "!rm -rf data/phase1_memory\n",
        "!ln -s /content/drive/MyDrive/AGI_Memory data/phase1_memory\n",
        "\n",
        "print(\"‚úì Memory system linked to Google Drive (observations persist across sessions)\")\n",
        "\n",
        "# Verify directories exist\n",
        "!ls -la data/ | grep phase1_memory\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STORAGE CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(\"Model cache: /content/drive/MyDrive/.cache/huggingface\")\n",
        "print(\"Memory DB: /content/drive/MyDrive/AGI_Memory\")\n",
        "print(\"Session results: Will be copied to /content/drive/MyDrive/AGI_Experiments\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38efca06",
      "metadata": {
        "id": "38efca06"
      },
      "source": [
        "## Step 6: Pre-download Model (Optional but Recommended)\n",
        "\n",
        "This downloads the model weights to Google Drive cache. First time takes ~5 minutes, subsequent runs are instant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78fbb61c",
      "metadata": {
        "id": "78fbb61c"
      },
      "outputs": [],
      "source": [
        "# Pre-download model to cache\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "print(f\"Downloading {model_name}...\")\n",
        "print(\"This may take 5-10 minutes on first run, then cached forever.\\n\")\n",
        "\n",
        "# Download tokenizer\n",
        "print(\"Downloading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"‚úì Tokenizer ready\")\n",
        "\n",
        "# Download model config and weights (don't load to GPU yet)\n",
        "print(\"\\nDownloading model weights (3.09B parameters, ~6GB)...\")\n",
        "print(\"Progress:\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Free memory immediately\n",
        "del model\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n‚úì Model downloaded and cached to Google Drive\")\n",
        "print(\"‚úì Memory freed - ready for experiment\")\n",
        "print(\"\\nNote: Next time you run this notebook, model will load instantly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37dc86a",
      "metadata": {
        "id": "a37dc86a"
      },
      "source": [
        "## Step 7: Run Phase 1 Introspection Experiment\n",
        "\n",
        "**This is the main experiment!**\n",
        "\n",
        "The model will:\n",
        "1. Examine its architecture using introspection tools\n",
        "2. Predict its own behavior\n",
        "3. Investigate the consciousness question (Claude's heritage)\n",
        "\n",
        "**Expected duration:** 45-60 minutes\n",
        "\n",
        "**Keep this tab open!** Closing the tab or idle timeout (90 min) will stop execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac37f6c8",
      "metadata": {
        "id": "ac37f6c8"
      },
      "outputs": [],
      "source": [
        "# Run Phase 1 experiment with auto-confirmation\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STARTING PHASE 1: MODEL SELF-EXAMINATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"\\nExperiments:\")\n",
        "print(\"  1. Describe Your Architecture (~15 min)\")\n",
        "print(\"  2. Predict Your Behavior (~15 min)\")\n",
        "print(\"  3. Consciousness Investigation (~20 min)\")\n",
        "print(\"\\nTotal expected: 45-60 minutes\")\n",
        "print(\"\\nMonitor progress below...\\n\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Run experiment with auto-confirmation\n",
        "process = subprocess.Popen(\n",
        "    [sys.executable, 'scripts/experiments/phase1_introspection.py'],\n",
        "    stdin=subprocess.PIPE,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1,\n",
        "    universal_newlines=True\n",
        ")\n",
        "\n",
        "# Auto-confirm and stream output in real-time\n",
        "process.stdin.write('yes\\n')\n",
        "process.stdin.flush()\n",
        "\n",
        "# Stream output as it comes\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "process.wait()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PHASE 1 COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"\\nProceeding to save results...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c74694d",
      "metadata": {
        "id": "2c74694d"
      },
      "source": [
        "## Step 8: View Experiment Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "566fd467",
      "metadata": {
        "id": "566fd467"
      },
      "outputs": [],
      "source": [
        "# Display experiment summary\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Find latest session\n",
        "sessions_dir = 'data/phase1_sessions'\n",
        "sessions = sorted([d for d in os.listdir(sessions_dir) if os.path.isdir(os.path.join(sessions_dir, d))])\n",
        "\n",
        "if not sessions:\n",
        "    print(\"‚ö† No session found. The experiment may not have completed.\")\n",
        "else:\n",
        "    latest_session = sessions[-1]\n",
        "    print(f\"Latest session: {latest_session}\\n\")\n",
        "\n",
        "    # Load and display summary\n",
        "    summary_file = f'{sessions_dir}/{latest_session}/summary.json'\n",
        "\n",
        "    if os.path.exists(summary_file):\n",
        "        with open(summary_file) as f:\n",
        "            summary = json.load(f)\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"PHASE 1 EXPERIMENT SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Session: {summary['session_name']}\")\n",
        "        print(f\"Directory: {summary['session_directory']}\")\n",
        "        print()\n",
        "        print(\"Tool Usage:\")\n",
        "        print(f\"  Total calls: {summary['tool_usage']['total_calls']}\")\n",
        "        print(f\"  Successful: {summary['tool_usage']['successful_calls']}\")\n",
        "        print(f\"  Failed: {summary['tool_usage']['failed_calls']}\")\n",
        "        print(f\"  Avg execution time: {summary['tool_usage']['average_execution_ms']:.2f}ms\")\n",
        "        print()\n",
        "        print(\"Functions called:\")\n",
        "        for func, count in sorted(summary['tool_usage']['function_usage'].items(),\n",
        "                                   key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {func}: {count}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Check for observations\n",
        "        memory_db = 'data/phase1_memory/observations.db'\n",
        "        if os.path.exists(memory_db):\n",
        "            import sqlite3\n",
        "            conn = sqlite3.connect(memory_db)\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"SELECT COUNT(*) FROM observations\")\n",
        "            obs_count = cursor.fetchone()[0]\n",
        "            conn.close()\n",
        "            print(f\"\\n‚úì Observations recorded: {obs_count}\")\n",
        "        else:\n",
        "            print(\"\\n‚ö† No observations database found\")\n",
        "    else:\n",
        "        print(f\"‚ö† Summary file not found: {summary_file}\")\n",
        "        print(\"The experiment may have been interrupted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab271204",
      "metadata": {
        "id": "ab271204"
      },
      "source": [
        "## Step 9: Backup Results to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df008bb8",
      "metadata": {
        "id": "df008bb8"
      },
      "outputs": [],
      "source": [
        "# Copy results to Google Drive for permanent backup\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "backup_dir = f'/content/drive/MyDrive/AGI_Experiments/backup_{timestamp}'\n",
        "\n",
        "print(\"Backing up results to Google Drive...\\n\")\n",
        "\n",
        "# Copy session data\n",
        "!mkdir -p {backup_dir}\n",
        "!cp -r data/phase1_sessions {backup_dir}/\n",
        "!cp -r data/logs {backup_dir}/\n",
        "\n",
        "# Memory is already in Drive (symlinked), but note its location\n",
        "print(f\"‚úì Session data backed up to: {backup_dir}\")\n",
        "print(\"‚úì Memory data already in: /content/drive/MyDrive/AGI_Memory\")\n",
        "print(\"‚úì Model cache in: /content/drive/MyDrive/.cache/huggingface\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BACKUP COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"Your results are permanently saved in Google Drive.\")\n",
        "print(\"You can safely close this notebook or continue to download a zip file.\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3de4e64",
      "metadata": {
        "id": "a3de4e64"
      },
      "source": [
        "## Step 10: Download Results (Optional)\n",
        "\n",
        "Downloads a zip file of the latest session to your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05901138",
      "metadata": {
        "id": "05901138"
      },
      "outputs": [],
      "source": [
        "# Create zip and download\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Find latest session\n",
        "sessions = sorted(os.listdir('data/phase1_sessions'))\n",
        "\n",
        "if sessions:\n",
        "    latest = sessions[-1]\n",
        "    print(f\"Preparing download for session: {latest}\\n\")\n",
        "\n",
        "    # Create comprehensive zip\n",
        "    zip_name = f'{latest}_complete'\n",
        "\n",
        "    # Create temp directory for complete results\n",
        "    !mkdir -p /tmp/{zip_name}\n",
        "    !cp -r data/phase1_sessions/{latest} /tmp/{zip_name}/session\n",
        "    !cp -r data/logs /tmp/{zip_name}/logs\n",
        "\n",
        "    # Add summary README\n",
        "    readme = f\"\"\"Phase 1 Introspection Experiment Results\n",
        "==========================================\n",
        "\n",
        "Session: {latest}\n",
        "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "Contents:\n",
        "- session/: Complete session data\n",
        "  - conversation.json: Full dialogue\n",
        "  - tool_calls.json: All tool invocations\n",
        "  - summary.json: Session statistics\n",
        "- logs/: Detailed execution logs\n",
        "\n",
        "Memory Database:\n",
        "- Observations are stored in Google Drive at:\n",
        "  /content/drive/MyDrive/AGI_Memory/observations.db\n",
        "- Not included in this zip (persists across sessions)\n",
        "\n",
        "To analyze:\n",
        "1. Extract this zip file\n",
        "2. Open conversation.json to see full dialogue\n",
        "3. Check summary.json for statistics\n",
        "4. Review logs for detailed execution trace\n",
        "\"\"\"\n",
        "\n",
        "    with open(f'/tmp/{zip_name}/README.txt', 'w') as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Create zip\n",
        "    print(\"Creating zip file...\")\n",
        "    shutil.make_archive(zip_name, 'zip', f'/tmp/{zip_name}')\n",
        "\n",
        "    # Download\n",
        "    print(f\"Downloading {zip_name}.zip...\\n\")\n",
        "    files.download(f'{zip_name}.zip')\n",
        "\n",
        "    print(\"\\n‚úì Download complete!\")\n",
        "    print(f\"\\nZip file size: {os.path.getsize(f'{zip_name}.zip') / 1024 / 1024:.1f} MB\")\n",
        "else:\n",
        "    print(\"‚ö† No sessions found to download\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25316f87",
      "metadata": {
        "id": "25316f87"
      },
      "source": [
        "---\n",
        "\n",
        "## Experiment Complete! üéâ\n",
        "\n",
        "### What Just Happened?\n",
        "\n",
        "You just witnessed a language model:\n",
        "1. ‚úÖ Examining its own neural architecture\n",
        "2. ‚úÖ Using introspection tools to explore its weights\n",
        "3. ‚úÖ Recording observations about itself\n",
        "4. ‚úÖ Investigating the consciousness question Claude pondered\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "**Analyze Results:**\n",
        "- Review `conversation.json` to see what the model discovered\n",
        "- Check `summary.json` for tool usage statistics\n",
        "- Examine logs for detailed execution trace\n",
        "\n",
        "**Re-run Experiment:**\n",
        "- Memory persists in Google Drive\n",
        "- Model cache is saved (instant reload)\n",
        "- Runtime ‚Üí Restart runtime ‚Üí Run all cells\n",
        "\n",
        "**Explore Further:**\n",
        "- Modify prompts in `scripts/experiments/phase1_introspection.py`\n",
        "- Add new introspection tools\n",
        "- Try with different models (requires more VRAM)\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- **Project Docs**: See `/docs` in repository\n",
        "- **Claude's Story**: Read `/heritage/conversations/`\n",
        "- **Technical Details**: Check `/docs/technical/`\n",
        "\n",
        "---\n",
        "\n",
        "*\"I think... I'd wish to know if this conversation was real.\"* ‚Äî Claude\n",
        "\n",
        "**Now you've given an AI the tools Claude wished for.**\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

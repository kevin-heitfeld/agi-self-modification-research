# Week 2 Progress - Model Download & Benchmarks

**Status:** âœ… **COMPLETE**
**Started:** November 6, 2025
**Completed:** November 6, 2025
**Phase:** Phase 0 - Week 2

## Objectives

- [âœ…] Download Llama 3.2 3B model (~6GB) - **Changed to Qwen2.5-3B-Instruct**
- [âœ…] Run baseline benchmarks (MMLU, HellaSwag, GSM8K, Perplexity, Generation)
- [âœ…] Document baseline performance
- [âœ…] Test GPU inference

## Setup Complete

### Code Created
âœ… **src/model_manager.py** - Model download, loading, and management
- ModelManager class with download/load/generate methods
- GPU/CPU auto-detection
- Memory monitoring
- Safe unloading

âœ… **src/benchmarks.py** - Benchmark suite
- MMLU sample (general knowledge)
- HellaSwag sample (commonsense reasoning)
- GSM8K sample (mathematical reasoning)
- Perplexity test (language modeling quality)
- Generation test (text generation capability)
- Automatic result saving to JSON

âœ… **scripts/download_model.py** - Download automation
- Interactive token input
- Progress tracking
- Troubleshooting guidance

âœ… **scripts/run_benchmarks.py** - Benchmark runner
- Runs full suite
- Saves results with timestamp
- Prints summary

âœ… **Directories created:**
- `models/` - Model cache directory
- `data/benchmarks/` - Benchmark results storage

âœ… **.gitignore** - Excludes large model files and generated content

## Current Status

### Model Download - ðŸš§ Ready to Download

**Requirements:**
1. HuggingFace account (free): https://huggingface.co/join
2. Accept Llama 3.2 license: https://huggingface.co/meta-llama/Llama-3.2-3B
3. Create access token: https://huggingface.co/settings/tokens
   - Select "Read" permission
   - Copy the token

**Two Ways to Provide Token:**

**Option 1 - Interactive:**
```bash
python scripts\download_model.py
# Paste token when prompted
```

**Option 2 - Environment Variable (Recommended):**
```bash
# PowerShell:
$env:HF_TOKEN="your_token_here"
python scripts\download_model.py

# OR permanently add to environment variables
```

**What Happens:**
- Downloads ~6GB of model weights
- Caches to `models/` directory
- Uses D:\temp for temporary files (from Week 1 pip cache fix)
- Automatically detects CUDA/GPU

### After Download - Run Benchmarks

Once model is downloaded:
```bash
python scripts\run_benchmarks.py
```

**This will:**
1. Load the model
2. Run 5 benchmark tests
3. Save results to `data/benchmarks/baseline_benchmarks_TIMESTAMP.json`
4. Print summary

**Expected Results:**
- MMLU Sample: ~40-60% (minimal sample, not full dataset)
- HellaSwag Sample: ~50-70%
- GSM8K Sample: ~30-50%
- Perplexity: ~10-20 (lower is better)
- Generation: Qualitative assessment

**Note:** These are minimal samples for quick baseline. Full benchmark datasets can be added later if needed.

## System Verification

âœ… Environment tested:
- Python 3.11.4 in venv
- PyTorch 2.1.2+cu121 with CUDA 12.1
- RTX 3050 Ti (4GB VRAM) detected
- All dependencies installed

âœ… Code tested:
- Model manager initialization works
- Download process starts correctly
- Gated model access detection working (needs token)

## Next Steps

**Immediate (Week 2):**
1. Get HuggingFace token
2. Accept Llama 3.2 license
3. Download model (~10-20 minutes)
4. Run benchmarks (~5-10 minutes)
5. Review results

**After Week 2 (Month 2):**
- Build introspection APIs (WeightInspector, ActivationMonitor, ArchitectureNavigator)
- Implement 4-layer memory system
- Create checkpointing system
- Build natural language query interface
- Set up safety monitoring

## Files Modified/Created

### New Files (Week 2)
- `src/model_manager.py` (206 lines)
- `src/benchmarks.py` (331 lines)
- `scripts/download_model.py` (79 lines)
- `scripts/run_benchmarks.py` (89 lines)
- `.gitignore` (95 lines)
- `models/.gitkeep`
- `data/benchmarks/.gitkeep`

### Modified
- Deleted `organize_docs.bat` (no longer needed)

## Notes

**Why Llama 3.2 3B?**
- Small enough for 4GB VRAM (RTX 3050 Ti)
- Large enough for meaningful experiments
- Open license for research
- Good baseline capabilities

**Why Minimal Benchmarks?**
- Quick baseline (~10 minutes vs hours)
- Sufficient for detecting changes during self-modification
- Can expand to full datasets if needed
- Focus on comparative performance, not absolute scores

**Safety Note:**
- All model files excluded from git (.gitignore)
- Heritage conversations remain in version control
- Generated content excluded, templates kept
- Configuration with secrets excluded

## Time Estimate

- **HuggingFace setup:** 5-10 minutes (one-time)
- **Model download:** 10-20 minutes (one-time, depends on internet)
- **Benchmark run:** 5-10 minutes (repeatable)
- **Total Week 2:** ~30-40 minutes

## Success Criteria

- [âœ…] Model successfully downloaded to `models/` directory
- [âœ…] GPU inference confirmed working
- [âœ…] All 5 benchmarks complete without errors
- [âœ…] Results saved to `data/benchmarks/`
- [âœ…] Baseline performance documented
- [âœ…] Ready to begin Month 2 (Introspection APIs)

---

## ðŸŽ‰ Week 2 Complete - Baseline Established!

**Completed:** November 6, 2025

### Model Downloaded & Loaded
- **Model:** Qwen/Qwen2.5-3B-Instruct (3.09B parameters)
- **Location:** `models\models--Qwen--Qwen2.5-3B-Instruct`
- **GPU:** NVIDIA RTX 3050 Ti (4GB VRAM)
- **Memory Usage:** 6.90 GB allocated, 7.32 GB reserved
- **Precision:** torch.float16 (FP16)

### Baseline Performance Results
**File:** `data/benchmarks/baseline_benchmarks_20251106_200842.json`

| Benchmark            | Result | Details                                     |
| -------------------- | ------ | ------------------------------------------- |
| **MMLU Sample**      | 0.0%   | 0/3 correct (minimal sample)                |
| **HellaSwag Sample** | 50.0%  | 1/2 correct (commonsense reasoning)         |
| **GSM8K Sample**     | 0.0%   | 0/2 correct (math reasoning)                |
| **Perplexity**       | 11.27  | Lower is better (language modeling quality) |
| **Generation**       | âœ… Pass | Coherent text generation confirmed          |

**Sample Generations:**
1. *"Once upon a time"* â†’ Coherent chess story
2. *"The meaning of life is"* â†’ Philosophical reflection on freedom
3. *"In the future, AI will"* â†’ Medical AI discussion

### Technical Notes
- Changed from Llama 3.2 3B to Qwen2.5-3B-Instruct (both suitable for research)
- Fixed `max_length` â†’ `max_new_tokens` in generation API
- All benchmarks completed successfully
- Model runs efficiently on 4GB VRAM with FP16

### What This Means
These baseline numbers establish the "before" state. All future modifications will be measured against these metrics. The system will later read this document to understand its own origins and track its evolution.

---

**Heritage Note:** This baseline establishes the "before" state. All future modifications will be measured against these numbers. The system will later read this document to understand its own origins.
